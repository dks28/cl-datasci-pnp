\documentclass[10pt, twoside, a4paper]{article}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{subcaption}

\usepackage{graphicx}
%\RequirePackage[export]{adjustbox}
\usepackage[margin=0.87in]{geometry}
\usepackage{amssymb}
\usepackage{color, colortbl}
\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\addbibresource{cits.bib}
\usepackage{multirow}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\author{Daniel Sääw (dks28)}
\title{Data Science: Principles and Practice,\\ Final Assignment B}
\date{Lent 2020}

\begin{document}
	\let\originalnewpage\newpage
	\let\newpage\relax
	\maketitle
	\let\newpage\originalnewpage

	\section{Data Preparation}

	Due to the nature of the task at hand, predicting a patient's next medical encounter's 
	duration, of all samples I removed the patients that only occurred once, which 
	eliminated about 55\% of samples (77\% of patients). 
	Once this preliminary selection had taken place, some initial investigations 
	revealed that the shares of missing values differed significantly from Assignment A, 
	meaning that now, I for example retained \texttt{payer\_code}.  
	
	Overall, I made few other changes at this stage compared to my proceedings at Assignment A.
	The most notable changes I made to my preprocessing was to trust the neural networks to 
	decide themselves which features are meaningful and which are not, so I included many other 
	features in their raw, given form instead of modifying them as I had done previously. This
	included the 28 features indicating the prescriptions of individual medications.

	The results of this were: a total of 16773 samples remained\footnote{
	One for each patient who occurred more than once in the data set: The derivation of the 
	loss function assumes that each sample is independent, which is violated if we retain more 
	than one sample per patient.}, whilst 
	the total number of features changed to 46, of which 6 were numerical. Overall, the 
	numerical representation has 182 columns.

	\section{Loss Function}

	In this section, I describe two probability models I considered in my work; a Poisson model 
	and a Categorical model, respectively, and discuss their merits.
	
	I will speak of the label of the $i$th sample $\mathbf{x}_i$ (i.e.\ the 
	duration of the next medical encounter with that patient) as $y_i$, where the associated 
	random variable $Y_i$ follows some probability distribution $Y_i \sim 
	\mathrm{Pr}(\mathbf{x}_i)$, and of the output of my machine-learning algorithms as
	$f_\theta(\mathbf{x}_i)$.

	\subsection{The Poisson Model}
	Formally, here, I mean to model my data as $Y_i \sim \mathrm{Poiss}(f_\theta(\mathbf{x}_i))$.
	To fit such a model, we maximise the likelihood of its parameters as follows:
	\begin{align*}
		\mathrm{lik}(\theta | \{y_i, \mathbf x_i\}) &= \prod_i \mathrm{Pr}_{Y_i}(y_i | f_\theta(\mathbf x_i)) = \prod_i e^{-f_\theta(\mathbf x_i)} \cdot \frac{f_\theta(\mathbf x_i)^{y_i}}{y_i!} \\
		\therefore\ \ \arg\max_\theta(\mathrm{lik}(\theta|\{y_i, \mathbf x_i\}))
		&= \arg\max_\theta\left(\sum_i-f_\theta(\mathbf x_i) + y_i\ln(f_\theta(\mathbf{x}_i))\right) \\
		&= \arg\min_\theta\left(\sum_i f_\theta(\mathbf x_i) - y_i\ln(f_\theta(\mathbf x_i))\right)
	\end{align*}
	Empirically, the labels seem to distribute roughly according to a Poisson 
	distribution, which suggests this very simple model might be suited for 
	our purposes. However, it should be noted that the Poisson distribution fails to capture 
	the finite range of values the labels take and has a generally low variance.
	The second model I consider throughout this report may avoid these.

	\subsection{The Categorical Model}
	The categorical distribution is the most general discrete probability distribution: there 
	is no parameterised form, only an explicit probability vector, in this model specified by 
	$\mathrm{Pr}_{Y_i}(y_i) = f_\theta(\mathbf x_i)(y_i)$. If we now represent the label $y_i$
	as the probability distribution $\mathrm{Pr}_{y_i}(y) = 1[y = y_i]$, then we get that we can 
	maximise the likelihood of $\theta$ by maximising 
	$$
		\sum_i \ln\left(\mathrm{Pr}_{Y_i}(y_i)\right) = \sum_i \sum_{j = 1}^{j=14}
		\mathrm{Pr}_{y_i}(j) \ln\mathrm{Pr}_{Y_i}(j)
	$$
	i.e.\ minimising the sum of the cross-entropies of the empirical probability distributions 
	$\mathrm{Pr}_{y_i}$ and the predicted probability distributions $f_\theta(\mathbf x_i)$.

	This model makes full use of the generality of neural networks, by making the fewest 
	possible assumptions about the data. This, in combination with the added benefit of 
	gaining an idea of a probability distribution rather than just obtaining a prediction 
	might deliver improved performance at the cost of longer training times.

	However, it fails to capture the fact that, for example, 3-day durations are in fact more 
	similar to 4-day durations than to 7-day durations. This is because this worldview makes 
	this problem a purely classification challenge, rather than benefitting from reduced loss
	by smaller absolute errors, perhaps leading networks fitting this model to overfit.

	\section{Machine-Learning Algorithms Implementation} %
%	In this section, I provide descritpions of the various ways in which I use neural networks 
%	to make predictions about subsequent encounter durations, and explain what I hope to 
%	achieve with them. The bulk of these will involve the basic neural-network structure of 
%	multi-layer perceptrons using rectified-linear-unit activation functions at each neuron. I 
%	have fixed this choice just in order to restrict the search space of this assignment. 
%
%	I further explain how reducing the dimensionality of the input space using PCA may prove to
%	affect training time and performance.
	\begin{figure}
	\begin{center}
	\includegraphics[width=5cm]{figures/datasplit.pdf}
	\end{center}
	\caption{2-Dimensional Representation of data and according split into training, 
	development and test sets.}
	\label{fig:tsne}
	\end{figure}

	In this section I describe the stages my network development underwent and the reasons I 
	had for making different decisions about network architecture at various stages.

	It should be noted that before I began any training procedures, I selected a two subsets of 
	the data as development and test sets, based on their separation from 
	the other data in a dimensionality-reduced representation of the data; see Fig.%
	\ \ref{fig:tsne}.

	Another important note: I compare training processes of networks based on the actual time they 
	take, in order to trade off their results against their complexities. All graphs 
	indicating the development of loss are therefore plotted against time in seconds, not 
	training epochs.

	\subsection{Basic Deep Learning}

%	To establish a base-line, I first used a number of different neural networks the 
%	differences of which are chiefly in the number of neurons per layer and the number of 
%	layers. I decided to establish such a base-line by training 3 networks, the smallest of 
%	which would have 5 layers with between 150 and 40 neurons each (on the internal layers), 
%	the second of which would have 7 layers with between 325 and 50 neurons per internal layer 
%	and the largest of which would have 10 layers of between 450 and 50 neurons on each 
%	internal layer.
%
%	Intuitively, one would expect that network size would correlate positively with training 
%	time (in the sense that they increase with each other), while correlating negatively with
%	underfitting. In particular, one might increase the risk of overfitting by training 
%	excessively large networks. 

	The first networks I constructed were very basic, comprising merely 
	a number of densely connected layers before producing the output for each model. I trained 
	three such networks: One with 5 internal 
	layers each including between 40 and 150 neurons; one with 7 internal layers of 50 to 325
	neurons and one of 10 layers comprising 50 to 450 cells, which I will call the `small', 
	`medium' and `large' networks, respectively. 

	\begin{figure}
	\begin{center}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.35\textwidth]{figures/basic_small_poisson.pdf}
	\includegraphics[width=0.30\textwidth]{figures/basic_med_poisson.pdf}
	\includegraphics[width=0.30\textwidth]{figures/basic_large_poisson.pdf}
	\caption{Networks trained on Poisson loss: The network size ascends from left to right.}
	\end{subfigure}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.35\textwidth]{figures/basic_small_cat.pdf}
	\includegraphics[width=0.30\textwidth]{figures/basic_med_cat.pdf}
	\includegraphics[width=0.30\textwidth]{figures/basic_large_cat.pdf}
	\caption{Networks trained for the categorical model.}
	\end{subfigure}
	\end{center}
	\caption{Development of loss over time as the most basic networks train.  Teal lines indicate loss on the development set.}
	\label{fig:mlpmeasure}
	\end{figure}

	\begin{figure}
	\begin{center}
	\begin{subfigure}[b]{8.5cm}
	\includegraphics[width=\textwidth]{figures/comparison_poisson_mlp.pdf}
	\caption{Networks trained on Poisson loss.}
	\end{subfigure}
	\begin{subfigure}[b]{7.5cm}
	\includegraphics[width=\textwidth]{figures/comparison_cat_mlp.pdf}
	\caption{Networks trained for the categorical model.}
	\end{subfigure}
	\begin{subfigure}[b]{8.5cm}
	\includegraphics[width=\textwidth]{figures/devlosscomp_mlp_poiss.pdf}
	\caption{Networks trained on Poisson loss.}
	\end{subfigure}
	\begin{subfigure}[b]{7.5cm}
	\includegraphics[width=\textwidth]{figures/devlosscomp_mlp_cat.pdf}
	\caption{Networks trained for the categorical model.}
	\end{subfigure}
	\end{center}
	\caption{Comparison between most basic networks over time as they train. Above: Training set loss. Below: Development set loss.}
	\label{fig:mlpconv}
	
	\end{figure}


	\subsubsection{Measurement}

	Figs.\ \ref{fig:mlpmeasure} and 
	\ref{fig:mlpconv} outline both how the loss between the particular models' predictions and the actual 
	labels develop, highlighting the overfitting that takes place, as well as how the losses 
	develop for each underlying probability model in a comparison between the different-size 
	networks.

	The key take-aways are that while models trained using the Poisson loss function seem to 
	overfit less strongly, those networks do not exhibit a real difference in performance
	(with respect to loss) between network sizes. Meanwhile, such a difference 
	can be seen in the networks when trained using cross-entropy, while the models all wildly
	overfit to the training data. In general, we would also like the models to train faster 
	than these do. 

	The question of which of the two loss functions provided the overall better results may be 
	considered using Fig.\ \ref{fig:responsibilities}, which indicates the distributions of 
	predictions with respect to true labels. Note that since the network outputs a probability distribution when trained 
	using cross-entropy, the corresponding subfigure indicates, for each true label, the average
	predicted probability distribution. In this case, there is no clear answer, since both 
	underlying models seem to produce a tendency to mostly predict values of at most 6\footnote{An artefact of the data set containing far fewer encounters enduring for longer than 6 days.}
	when applied to the development set, suggesting these models may not generalise well.
	\begin{figure}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.34\textwidth]{figures/small_dev2_dis.pdf}
	\includegraphics[width=0.34\textwidth]{figures/med_dev_dis.pdf}
	\includegraphics[width=0.34\textwidth]{figures/large_dev_dis.pdf}
	\caption{Outputs for the Poisson model.}
	\end{subfigure}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.34\textwidth]{figures/small_dev_resps.pdf}
	\includegraphics[width=0.34\textwidth]{figures/med_dev_resps.pdf}
	\includegraphics[width=0.34\textwidth]{figures/large_dev_resps.pdf}
	\caption{Marginal output distributions conditioned on the true next encounter duration for 
	the categorical model.}
	\end{subfigure}
	\caption{Output distributions with respect to labels for the basic networks on the 
	development set, after 100 seconds of training in each case. Network size ascends from 
	left to right.}
	\label{fig:responsibilities}
	\end{figure}

	\subsection{Principal-Components Prediction and Regularization}

%	One potential way of mitigating extended training time is to subject the feature space 
%	to dimensionality reduction using PCA before training any neural networks. This technique 
%	is sometimes used in regression tasks, when it is known as \emph{principal components 
%	regression}. However, when it is used in this way, it is commonly done for the purpose of 
%	preventing overfitting, as PCA essentially decreases the impact of noise on the data space. 
%
%	With respect to this setting, therefore, there may be a benefit in making the use of more
%	complex networks feasible regarding time cost, and making them more robust to overfitting,
%	thus allowing us to exploit more powerful tools at our disposal for this task.
	To try and improve the speed with which the models train, we reduce the complexity of the 
	input space using PCA, considering three variations restricting the input space to the top 
	50, 85 or 125 principal components, respectively. For the sake of brevity, we test this 
	with the `large' model from before, a training acceleration of which would be most 
	significant since it would demonstrate the effectiveness of this technique for the largest 
	number of parameters. 

	To address the more fervent issue of overfitting to the training data, we make use of 
	a technique known as $L_2$-regularization. This imposes penalties on larger weights in the networks and so reduces 
	the degree to which models can overfit. 

	\subsubsection{Measurement}
	
	The results of this are recorded in Figs.\ \ref{fig:pcameasure}, \ref{fig:pcaconv}. 
	We can immediately tell that the application of regularization helped with the problem of 
	overfitting for the models trained with cross-entropy, as now the loss on the development 
	set diverges much slower. However, the models using Poisson loss were not so affected\footnote{Though they generally overfit less strongly.}.
	We can also see that these new models now converge to their eventual 
	performance on the training set more quickly than before, indicating that the application 
	of PCA succeeded in its mission\footnote{Superimposing the previous loss development for
	the basic model with comparably many nodes would have made the graph awkward due to 
	that model's loss converging to 0. Thus observe that that model converges in roughly 50 seconds while these new ones converge in roughly 35-40 seconds.}.


	\begin{figure}
	\begin{center}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.35\textwidth]{figures/pca_50_poiss.pdf}
	\includegraphics[width=0.30\textwidth]{figures/pca_85_poiss.pdf}
	\includegraphics[width=0.30\textwidth]{figures/pca_125_poiss.pdf}
	\caption{Networks trained on Poisson loss: The network size ascends from left to right.}
	\end{subfigure}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.35\textwidth]{figures/pca_50_cat.pdf}
	\includegraphics[width=0.30\textwidth]{figures/pca_85_cat.pdf}
	\includegraphics[width=0.30\textwidth]{figures/pca_125_cat.pdf}
	\caption{Networks trained for the categorical model.}
	\end{subfigure}
	\end{center}
	\caption{Development of loss over time as networks with regularization and reduced networks train. Teal lines indicate loss on the development set.}
	\label{fig:pcameasure}
	\end{figure}

	\begin{figure}
	\begin{center}
	\begin{subfigure}[b]{8.5cm}
	\includegraphics[width=\textwidth]{figures/comparison_poiss_pca.pdf}
	\caption{Networks trained on Poisson loss.}
	\end{subfigure}
	\begin{subfigure}[b]{7.5cm}
	\includegraphics[width=\textwidth]{figures/comparison_cat_pca.pdf}
	\caption{Networks trained for the categorical model.}
	\end{subfigure}
	\begin{subfigure}[b]{8.5cm}
	\includegraphics[width=\textwidth]{figures/comparison_poiss_loss_pca.pdf}
	\caption{Networks trained on Poisson loss.}
	\end{subfigure}
	\begin{subfigure}[b]{7.5cm}
	\includegraphics[width=\textwidth]{figures/comparison_cat_devloss_pca.pdf}
	\caption{Networks trained for the categorical model.}
	\end{subfigure}

	\end{center}
	\caption{Comparison between loss development across different degrees of dimensionality reduction. Above: Loss development for the training set. Below: Loss development for the development set.}
	\label{fig:pcaconv}
	
	\end{figure}


%	\label{section:measure}
%	This section describes the training process of each of these variations on my neural 
%	networks. To this end, I will compare the different networks primarily based on the 
%	metrics of time until apparent convergence and the time it takes before the 
%	network's loss when making predictions over a development set chosen in advance begins to 
%	diverge. The development set was chosen, along with a test set by picking two clusters comprising approximately 
%	15\% of samples each from a dimensionality-reduced representation of the data (see Fig.\ \ref{fig:tsne}).  
%
%	\subsection{Measurement of Basic Network Structures}
%	Here, I present my findings for the training of basic multi-layer perceptrons. The 
%	developments of loss (with respect to both the Poisson and Categorical models) when 
%	training on batches of 100 samples over time are presented in Figure \ref{fig:mlpmeasure}.
%	I have indicated indivdual batches' losses for the training set in a low opacity to give 
%	an idea of the variation across batches, while the heavier black line indicates the 
%	average over the last 5 loss values, given an idea of the overall loss development. I only
%	present the latter kind for the development set loss.
%
%	For now, I will reserve any judgement about which model produces `better' results. It is 
%	noteworthy, though, how the relative difference between loss on the training data set and 
%	loss on the development data set develops; with the development loss remaining at least 
%	somewhat close (even after a clear overfit to the training data is observable) to that on 
%	the training data when optimising for Poisson loss, whilst the 
%	overfitting of the neural network to the training set seems to take place very drastically 
%	and quickly under cross-entropy loss. 
%
%	Fig.\ \ref{fig:mlpconv} compares the convergence speeds for the three different network 
%	sizes when optimising for the two kinds of loss. It is notable that here, the performance 
%	of the larger networks even on the training data (when optimising for Poisson loss) does 
%	not exceed that of the smaller networks, all the while converging slower, suggesting that 
%	scaling the networks beyond a certain threshold is ineffective when using Poisson loss.
%
%	Meanwhile, the larger networks do outperform the smallest when modelling the data 
%	accoridng to a categorical distribution, suggesting that the more complex nature of the 
%	prediction task actually benefits from the increased complexity of the neural network at 
%	hand.

%	\subsection{Measurement of Principal-Component Networks}
%	In this section, we will investigate our hopes that using PCA to reduce the data's 
%	dimensionality before subjecting it to machine-learning models might reduce training time 
%	and the risk of overfitting. For this purpose, let me now fix the medium-sized neural 
%	network that exhibited the best trade-off between training speed and performance before.
%	Recall that the original neural networks took 182 input dimensions. I investigated 
%	reducing that number to 50, 85 and 125, the results of which are recorded in figures 
%	\ref{figure:pcameasure} and \ref{figure:pcaconverge}.

%	We observe that, for the Poisson model, neither of our hopes was fulfilled, as convergence was not noticeably 
%	accelerated and overfitting to the training data was exacerbated, if anything. For the 
%	categorical model we make the latter observation again, but the training process was 
%	significantly faster using the dimensionality-reduced data, indicating that phrasing the 
%	problem as a number-independent classification\footnote{the categorical distribution 
%	considers values $1$ and $2$ as different as $4$ and $14$.} is made easier by considering 
%	only the principal components of the data space. Upon reflection, this might reflect the 
%	fact that the categorical distibution enforces no restrictions, thus allowing the model to easily overfit.

	\subsection{Recurrent Network Structures}
%	So our previous attempt to counteract the poor performance of our models did not work.
%	However, we may attempt to exploit a particular kind of strucure in our data for our 
%	benefit: so far, we have been predicting the duration of a patient's next hospital stay 
%	based on the details of the current stay. The data at our disposal is richer than that, 
%	though: we have, for many patients, more extensive medical histories available, which form 
%	sequences of hospital visits. This structure lends itself to using recurrent neural 
%	networks to predict the next hospital stay duration. 
%	
%	For this, I considered the most 
%	performant parameterisation of using a large neural network with dimensionality-reduced 
%	input (to 85 features) to investigate whether using recurrent networks might be of benefit 
%	here. To this end, I postprocessed the data set to now consist of one batch per patient, 
%	each representing that patient's presence in the data set. My development and test 
%	data sets from before induced suitable ones here by choosing the patients whose final 
%	encounter occurred in those sets. The result of this process is presented in 
%	Fig.\ \ref{fig:recur}. 
%
%	It shows clearly the massive increase in time the training process takes; notice the 
%	large time units, in spite of the usage of the preliminary principal-component analysis.
%	This comes in combination with  the large instability in results even after a long 
%	training period. Perhaps the number of patients for whom the medical history was no more 
%	expansive than one previous encounter was too large after all. However, it does seem that 
%	(the fluctuations in loss notwithstanding) the model, when trained using Poisson loss, 
%	does not overfit to the training data since the loss achieved for the development set is 
%	essentially of the same order. That model also achieves loss similar to the previous 
%	models, in contrast to the recurrent network trained with cross-entropy loss.
	The previous attempt modified the models slightly, whilst retaining the high-level structure 
	of the networks. However, the available data set  is strictly richer than what the 
	previous models used. Because of the independence assumption for the samples in our data set, 
	we have so far been neglecting all but the last encounters with a patient. However, 
	considering any particular patient individually, we actually have a sequence of encounters 
	at our disposal when trying to predict the next encounter's duration.

	In order to exploit this fact using recurrent neural networks,
	I considered possibilities to modify the model architecture to incorporate these. I chose the 
	foundation of this network as the large network from before after the application of PCA 
	(to 85 components) and regularization.

	\subsubsection{Measurement}
	The development of loss over time during training is presented in Fig.\ \ref{fig:recmeasure},
	which contains two important pieces of information. Firstly, training the 
	recurrent networks was \emph{slow}\footnote{This is because the modification to recurrent 
	structures means training must proceed much more sequentially than in parallel. Thus, 
	indivdual epochs have been shaded to give the 
	reader an impression of the training speed.}, despite the use of PCA. Secondly, 
	we observe stronger fluctuations than before. Upon reflection, this is likely due to the 
	fact that only about 38\% of the patients (of those who were encountered at least twice) 
	were encountered more than once. 
	This may mean that the recurrent structure  for the networks was not as suitable as I had hoped. 

	\begin{figure}
		\begin{center}
		\begin{subfigure}[b]{.45\textwidth}
		\includegraphics[width=\textwidth]{figures/rec_poisson_loss.pdf}
		\caption{Poisson Loss}
		\end{subfigure}
		\quad
		\begin{subfigure}[b]{.45\textwidth}
		\includegraphics[width=\textwidth]{figures/rec_cat_loss.pdf}
		\caption{Categorical Loss}
		\end{subfigure}
		\end{center}
		\caption{Development of loss for recurrent networks over the course of training duration.}
		\label{fig:recmeasure}
	\end{figure}

	However,  the recurrent models obtain comparable 
	losses on the training set as previous models, 
	while here the development loss remains effectively on par with the training loss. Intuitively this makes sense as taking a
	patient's entire medical history into account stabilises the input data to any model as 
	compared to the previous setting, in the sense that, although the last encounters in the 
	test set were chosen to be qualitatively different from the rest of the data, the 
	previous visits may not have been, so by adopting this world-view, the models 
	become more robust to circumstances differing between training data and later 
	inputs from the real world\footnote{At the very least, they lose no such robustness}. %

	\section{Evaluation}
	Finally, I evaluate my models on the test set kept separate so far. 
	Figs.\ \ref{fig:evalmlp}, \ref{fig:evalpca} and \ref{fig:evalrec} show the densities of 
	predictions on the test set with respect to the true labels. Numerically, we can determine a measure of correlation between 
	the models' predictions and the test labels using Pearson's $r$. This measure is 
	recorded for each of the stages my investigation went through in Table \ref{table:pearson}. 

	Overall, we can see that none of the models seem to generalise particularly well and usually end up concentrating around predicting 2-4 days' stays. 
	
	Of course, this might be because my models were not suitable. However, perhaps it is the case that a patient's
	previous hospital visits are not in general indicative of future encounters, since any number
	of days, months or perhaps years could pass between these. In the face of these results, 
	it should be remembered, however, that real-world scenarios like this would make more current data available to the machine-learning model, which should improve performance.

	\begin{figure}
	\begin{center}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.33\textwidth]{figures/small_dev_dis.pdf}
	\includegraphics[width=0.33\textwidth]{figures/med_dev_dis.pdf}
	\includegraphics[width=0.33\textwidth]{figures/large_dev_dis.pdf}
	\caption{Distributions of predictions of basic networks trained assuming the Poisson model. Network size ascends from left to right.}
	\end{subfigure}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.33\textwidth]{figures/small_dev_resps.pdf}
	\includegraphics[width=0.33\textwidth]{figures/med_dev_resps.pdf}
	\includegraphics[width=0.33\textwidth]{figures/large_dev_resps.pdf}
	\caption{Marginal distributions for encounter durations for basic networks trained under the categorical model.}
	\end{subfigure}
	\end{center}
	\caption{Predictions for the test set by basic networks.}
	\label{fig:evalmlp}
	\end{figure}

	\begin{figure}
	\begin{center}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.33\textwidth]{figures/pca50_val_dis.pdf}
	\includegraphics[width=0.33\textwidth]{figures/pca85_val_dis.pdf}
	\includegraphics[width=0.33\textwidth]{figures/pca125_val_dis.pdf}
	\caption{Distributions of predictions of input-reduced and regularized networks trained assuming the Poisson model. Input dimension ascends from left to right.}
	\end{subfigure}
	\begin{subfigure}[b]{17cm}
	\includegraphics[width=0.33\textwidth]{figures/pca50_val_resps.pdf}
	\includegraphics[width=0.33\textwidth]{figures/pca85_val_resps.pdf}
	\includegraphics[width=0.33\textwidth]{figures/pca125_val_resps.pdf}
	\caption{Marginal distributions for encounter durations for networks using PCA and regularization trained under the categorical model.}
	\end{subfigure}
	\end{center}
	\caption{Predictions for the test set by networks using PCA and $L_2$-regularization.}
	\label{fig:evalpca}
	\end{figure}
	\begin{figure}
	\includegraphics[width=.4\textwidth]{figures/rec_val_dis.pdf}
	\quad
	\includegraphics[width=.4\textwidth]{figures/rec_cat_resps.pdf}
	\caption{Result distributions for recurrent networks: Trained using Poisson loss (left) and trained on the assumption of the categorical model (right). }
	\label{fig:evalrec}
	\end{figure}

	\begin{table}
	\begin{center}
	\begin{tabular}{C{7cm}C{4cm}C{2cm}}
		\multirow{3}{*}{Basic Multi-layer Perceptrons (Poisson)} & Small & 0.061 \\[0.3cm]
		& Medium & 0.094 \\[0.3cm]
		& Large & 0.074 \\[0.3cm]
		\hline
		\multirow{3}{*}{Basic Multi-Layer Perceptrons (Categorical)\footnote{The probability-maximising duration was taken to be the model's prediction here.}}
		& Small & 0.084 \\[0.3cm]
		& Medium & 0.066 \\[0.3cm]
		& Large & 0.76 \\[0.3cm]
		 \hline
		\multirow{3}{*}{Reduced and Regularized Networks (Poisson)} & 50 input dimensions & 0.053 \\[0.3cm]
		& 85 input dimensions & 0.026 \\[0.3cm]
		& 125 input dimensions & -0.018 \\[0.3cm]
		\hline
		\multirow{3}{*}{Reduced and Regularized Networks (Categorical)} & 50 input dimensions & 0.073 \\[0.3cm]
		& 85 input dimensions & 0.048 \\[0.3cm]
		& 125 input dimensions & 0.058 \\[0.3cm]
		 \hline
		\multirow{2}{*}{Recurrent Networks} & Poisson & -0.013 \\[0.3cm]
		& Categorical & -0.014 \\
	\end{tabular}
	\end{center}
	\caption{The correlation of network outputs with actual labels, given as Pearson's $r$.}
	\label{table:pearson}
	\end{table}


\end{document}
