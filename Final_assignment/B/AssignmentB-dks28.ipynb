{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment B\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "This should not be exaggerated: We need to accomplish the following:\n",
    " - decide which columns to keep\n",
    " - decide which rows to keep\n",
    " - decide how to transmute certain features to increase information content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','sans-serif':['Computer Modern Roman']})\n",
    "## for Palatino and other serif fonts use:\n",
    "#rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../diabetes/diabetic_data_original.csv'\n",
    "\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "\n",
    "\"\"\"\n",
    "oh dear... all the null values in this data set are just placeholders, like ? in weight....\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.patient_nbr.value_counts() ).value_counts()/len(data.patient_nbr.value_counts())\n",
    "# We will have to drop more than half of rows exclusively due to sample isolation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data.patient_nbr.value_counts()\n",
    "inds_kept = data.patient_nbr.map(lambda x: counts[x] > 1).values\n",
    "reduced_data = data[inds_kept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.time_in_hospital.value_counts() # Excellent, no missing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(reduced_data.payer_code.value_counts())/len(reduced_data)\n",
    "1-(reduced_data.patient_nbr.value_counts() > 1).value_counts()/len(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_diag(diag):\n",
    "    if (type(diag) == float and np.isnan(diag)) or diag == \"NaN\" or diag == '?':\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        num = int(diag)\n",
    "    except Exception as e:\n",
    "        if diag[0] == 'E':\n",
    "            return \"SUPP_CLASS_EXTERNAL_CAUSE\"\n",
    "        elif diag[0] == 'V':\n",
    "            return \"SUPP_CLASS_HEALTH_FACTORS\"\n",
    "        else:\n",
    "            try:\n",
    "                num = float(diag)\n",
    "                return \"DIABETES\"\n",
    "            except Exception as ex:\n",
    "                return \"ERROR\"\n",
    "    if (num >= 390 and num <= 459) or num == 785:\n",
    "        return \"CIRCULATORY\"\n",
    "    elif (num == 250):\n",
    "        return \"DIABETES\"\n",
    "    elif (num >= 460 and num <= 519) or num == 786:\n",
    "        return \"RESPIRATORY\"\n",
    "    elif (num >= 520 and num <= 579) or num == 787:\n",
    "        return \"DIGESTIVE\"\n",
    "    elif num >= 800: \n",
    "        return \"INJURY_OR_POISON\"\n",
    "    elif num >= 710 and num <= 739:\n",
    "        return \"MUSKOSKELETAL\"\n",
    "    elif (num >= 580 and num <= 629) or num == 788:\n",
    "        return \"GENITOURINARY\"\n",
    "    elif num >= 140 and num <= 239:\n",
    "        return \"NEOPLASMS\"\n",
    "    elif num >= 680 and num <= 709:\n",
    "        return \"SKIN\"\n",
    "    elif num >= 780 and num <= 799:\n",
    "        return \"ILL-DEFINED\"\n",
    "    elif num <= 139:\n",
    "        return \"INFECTIOUS_PARASITIC\"\n",
    "    elif num >= 240 and num <= 279:\n",
    "        return \"ENDOCRINE_METABOLIC\"\n",
    "    elif num >= 290 and num <= 319:\n",
    "        return \"MENTAL_DISORDER\"\n",
    "    elif num >= 630 and num <= 679:\n",
    "        return \"PREGNANCY_COMPLICATIONS\"\n",
    "    elif num >= 280 and num <= 289:\n",
    "        return \"BLOOD\"\n",
    "    elif num >= 320 and num <= 389:\n",
    "        return \"NERVOUS_SENSE\"\n",
    "    elif num >= 740 and num <= 759:\n",
    "        return \"CONGENITAL\"\n",
    "    else: \n",
    "        return \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Changes from assignment A: \n",
    "# - no \"boolean\" glucose serum test\n",
    "# - not dropping rows w/ missing race data, but masking the missing values\n",
    "# - no dropping of particular discharge dispositions\n",
    "# - no dropping of payer code and medical specialty (just use masking)\n",
    "\n",
    "def cat_ads(asid):\n",
    "    if asid in [17,20,9]:\n",
    "        return \"NULL\"\n",
    "    elif asid in [1,2,3]:\n",
    "        return \"REFERRAL\"\n",
    "    elif asid in [4,5,6]:\n",
    "        return \"TRANSFER\"\n",
    "    elif asid in [7]:\n",
    "        return \"E.R.\"\n",
    "    elif asid in [8]:\n",
    "        return \"LAW_ENFORCEMENT\"\n",
    "    else:\n",
    "        return \"OTHER\"\n",
    "    \n",
    "def cat_adt(atid):\n",
    "    cats = [0, \"Emergency\",\"Urgent\",\"Elective\",\"Newborn\", \"NULL\", \"NULL\", \"Trauma Center\", \"NULL\"]\n",
    "    return cats[atid]\n",
    "\n",
    "def cat_dsch(did):\n",
    "    if did in [18,25,26]:\n",
    "        return \"NULL\"\n",
    "    elif did in [3,4,5]:\n",
    "        return \"ICF/SNF\"\n",
    "    elif did in [6,8,12]:\n",
    "        return \"FURTHER_CARE_HOME\"\n",
    "    elif did in [7]:\n",
    "        return \"LEFT_AMA\"\n",
    "    elif did in [15,17,9]:\n",
    "        return \"CARE_CONTINUES_IN_THIS_HOSPITAL\"\n",
    "    elif did in [13,14]:\n",
    "        return 'HOSPICE'\n",
    "    elif did in [11, 19, 20, 21]:\n",
    "        return 'EXPIRY'\n",
    "    elif did in [1]:\n",
    "        return 'WENT_HOME'\n",
    "    else:\n",
    "        return \"CARE_CONTINUES_ELSEWHERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in reduced_data.columns:\n",
    "    print(col)\n",
    "    counts = reduced_data[col].value_counts()\n",
    "    if '?' in counts.index:\n",
    "        print(counts['?'], f\"({counts['?']/len(reduced_data)})\")\n",
    "    else:\n",
    "        print('No missing values')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.A1Cresult.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_visit_duration(encounter_id):\n",
    "    try:\n",
    "        patient_id = reduced_data.loc[reduced_data.encounter_id == encounter_id].patient_nbr.values[0]\n",
    "        encounters = reduced_data.loc[reduced_data.patient_nbr == patient_id]\n",
    "        encounters = encounters.loc[encounters.encounter_id > encounter_id]\n",
    "        return encounters['time_in_hospital'].values[0]\n",
    "    except Exception:\n",
    "        return float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data['next_encounter_duration'] = reduced_data.encounter_id.map(get_next_visit_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.next_encounter_duration.value_counts().sum(), len(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data['discharge_disposition_cat'] = reduced_data.discharge_disposition_id.map(cat_dsch)\n",
    "reduced_data['discharge_disposition_id'] = reduced_data.discharge_disposition_id.map(str)\n",
    "\n",
    "reduced_data.age = reduced_data.age.map(lambda x: int(x[1]))\n",
    "\n",
    "reduced_data['admission_source_id'] = reduced_data.admission_source_id.map(int)\n",
    "reduced_data['admission_source_cat'] = reduced_data.admission_source_id.map(cat_ads)\n",
    "reduced_data['admission_source_id'] = reduced_data.admission_source_id.map(str)\n",
    "\n",
    "reduced_data['admission_type_id'] = reduced_data.admission_type_id.map(int)\n",
    "reduced_data['admission_type_cat'] = reduced_data.admission_type_id.map(cat_adt)\n",
    "reduced_data['admission_type_id'] = reduced_data.admission_type_id.map(str)\n",
    "\n",
    "generics = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', \n",
    "            'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', \n",
    "            'miglitol', 'troglitazone', 'tolazamide', 'examide', 'insulin', 'glyburide-metformin', \n",
    "            'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', \n",
    "            'metformin-pioglitazone']\n",
    "\n",
    "reduced_data['UP'] = np.sum([reduced_data[g] == 'Up' for g in generics], axis=0)\n",
    "reduced_data['STEADY'] = np.sum([reduced_data[g] == 'Steady' for g in generics], axis=0)\n",
    "reduced_data['DOWN'] = np.sum([reduced_data[g] == 'Down' for g in generics], axis=0)\n",
    "reduced_data['NO'] = np.sum([reduced_data[g] == 'No' for g in generics], axis=0)\n",
    "\n",
    "#reduced_data['age'] = reduced_data.age.map(lambda a: int(a[1]))\n",
    "\n",
    "reduced_data['diagnoses'] = list(map(list, reduced_data[['diag_1', 'diag_2', 'diag_3']].values))\n",
    "reduced_data['diagnoses'] = reduced_data.diagnoses.map(lambda l:[cat_diag(i) for i in l if not (type(i)==float and np.isnan(i))])\n",
    "\n",
    "reduced_data['total_visits'] = reduced_data.number_emergency + reduced_data.number_inpatient + reduced_data.number_outpatient\n",
    "\n",
    "reduced_data.drop(['weight'], axis=1)\n",
    "reduced_data.dropna(subset=['next_encounter_duration'], inplace=True)\n",
    "\n",
    "reduced_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_simple = ['race', \n",
    "                  'gender', \n",
    "                  'A1Cresult', \n",
    "                  'change', \n",
    "                  'diabetesMed',\n",
    "                  'discharge_disposition_cat',\n",
    "                  'admission_type_cat',\n",
    "                  'admission_source_cat',\n",
    "                  'medical_specialty',\n",
    "                  'readmitted'\n",
    "                 ] +generics\n",
    "\n",
    "poly_hot = ['diagnoses']\n",
    "\n",
    "\n",
    "# Encoding for numerical data:\n",
    "\n",
    "norm_simple = ['age']\n",
    "\n",
    "std_simple = ['time_in_hospital', \n",
    "                'num_lab_procedures', \n",
    "                'num_procedures', \n",
    "                'num_medications', \n",
    "                'total_visits'] # , 'UP', 'STEADY', 'DOWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reduced_data.medical_specialty.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "class LabelBinarizerDitchNull(TransformerMixin):\n",
    "    def __init__(self, num_features, *args, **kwargs):\n",
    "        self.n = num_features\n",
    "        self.encoders = []\n",
    "        for _ in range(self.n):\n",
    "            self.encoders.append(LabelBinarizer(*args, **kwargs))\n",
    "    def fit(self, X, y=0):\n",
    "        for i in range(self.n):\n",
    "            self.encoders[i].fit(X[:, i])\n",
    "        return self\n",
    "    def transform(self, X, y=0):\n",
    "        samples, n = X.shape\n",
    "        assert n == self.n\n",
    "        reses = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            resi = encoder.transform(X[:, i])\n",
    "            if \"XXXXXXXXXXX\" in encoder.classes_:\n",
    "                inds = encoder.classes_ != \"NULL\"\n",
    "                resi = resi[:, inds]\n",
    "            reses.append(resi)\n",
    "        num_new_features = sum([len(Z.T) for Z in reses])\n",
    "        res = np.zeros((samples, num_new_features))\n",
    "        i = 0\n",
    "        for Z in reses:\n",
    "            m, k = Z.shape\n",
    "            assert m == samples\n",
    "            res[:, i:i+k] = Z\n",
    "            i += k\n",
    "        return res\n",
    "    \n",
    "class PolyBinarizer(TransformerMixin):\n",
    "    def __init__(self, num_features, *args, **kwargs):\n",
    "        self.n = num_features\n",
    "        self.encoders = []\n",
    "        for _ in range(self.n):\n",
    "            self.encoders.append(dict())\n",
    "    def fit(self, X, y=0):\n",
    "        transforms = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            encoder['cats'] = sorted(np.unique(np.sum(X[:, i])))\n",
    "            def itransform(X):\n",
    "                samples = len(X)\n",
    "                res = np.zeros((samples, len(encoder['cats'])))\n",
    "                for j,x in enumerate(X):\n",
    "                    for cat in x:\n",
    "                        ind = encoder['cats'].index(cat)\n",
    "                        res[j, ind] += 1\n",
    "                return res\n",
    "            transforms.append(lambda X : itransform(X))\n",
    "        for i in range(self.n):\n",
    "            self.encoders[i]['transform'] = transforms[i]\n",
    "        return self\n",
    "    def transform(self, X, y=0):\n",
    "        samples, n = X.shape\n",
    "        assert n == self.n\n",
    "        reses = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            resi = encoder['transform'](X[:, i])\n",
    "            reses.append(resi)\n",
    "        num_new_features = sum([len(Z.T) for Z in reses])\n",
    "        res = np.zeros((samples, num_new_features))\n",
    "        i = 0\n",
    "        for Z in reses:\n",
    "            m, k = Z.shape\n",
    "            res[:, i:i+k] = Z\n",
    "            i += k\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_pipe_ditch_null = Pipeline([\n",
    "    ('selector', DataFrameSelector(one_hot_simple)),\n",
    "    ('label_binarizer', LabelBinarizerDitchNull(num_features=len(one_hot_simple))),\n",
    "])\n",
    "\n",
    "poly_hot_pipe = Pipeline([\n",
    "    ('selector', DataFrameSelector(poly_hot)),\n",
    "    ('label_binarizer', PolyBinarizer(num_features = len(poly_hot)))\n",
    "])\n",
    "\n",
    "norm_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(norm_simple)),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "std_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(std_simple)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"norm_pipeline\", norm_pipeline),\n",
    "        (\"std_pipeline\", std_pipeline),\n",
    "        (\"poly_hot_pipe\", poly_hot_pipe), \n",
    "        (\"oh_pipe_ditch_null\", oh_pipe_ditch_null),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data[generics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = full_pipeline.fit_transform(reduced_data)\n",
    "prepared_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prepared_data\n",
    "y = reduced_data.next_encounter_duration.values\n",
    "\n",
    "X.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - np.min(X) + 1e-7\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loss Function\n",
    "\n",
    "This part should be very brief. We need to decide on a suitable 'loss funtion', that is, we need to decide on a probabilistic model we would like to model our data on, and we find out what function we would like to maximise wrt its parameters based on features $x$, and a label $y$.\n",
    "\n",
    "The briefing text wsuggests the Poisson distribution, i.e. $Y_i \\sim \\mathrm{Poiss}(f_\\theta(x_i))$. However, the support of the Poisson distribution is the set of natural numbers, we care about a finite support because of the way that `time_in_hospital` was measured (capping its value at 14 and giving it minimum value 1). Therefore, I suggest the model $Y_i - 1 \\sim \\mathrm{B}(13, (f_\\theta(x_i)-1)/13)$ (a binomial distribution with 13 trials and expectation $f_\\theta(x_i) - 1$), which will give $Y_i$ a support of $[14] = \\{1, ... , 14\\}$.\n",
    "\n",
    "In order to model this, we will want to maximise the following:\n",
    "\n",
    "$$\n",
    "    \\sum_i (y_i - 1) \\log\\left(f_\\theta(x_i)-1\\right) + (14-y_i)\\log\\left(14 - f_\\theta(x_i)\\right)\n",
    "$$\n",
    "\n",
    "with respect to the parameters $\\theta$ of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Machine Learning Algorithms Implementation\n",
    "\n",
    "In this part, we actually build and train our neural nets. For this, we will need to define the loss function we derived above in Python, and choose an optimiser, in addition to playing around with the networks themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3.1: Simple neural nets...\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def lossfn (y, y_): \n",
    "    return -K.sum((y-1.0)*K.log(y_+K.epsilon()) + (14.0-y)*K.log(14-y_+K.epsilon()), axis=-1)\n",
    "\n",
    "ytrue, ypred = tf.constant([1,6,1,4], dtype=float), tf.constant([2,3,2,3],dtype=float)\n",
    "\n",
    "print(keras.losses.poisson(ytrue, ypred).numpy())\n",
    "lossfn(ytrue, ypred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ins = keras.layers.Input(shape=(121,))\n",
    "x = keras.layers.Dense(300)(ins)\n",
    "x = keras.activations.tanh(x)\n",
    "x = keras.layers.Dense(250)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(100)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x_ = keras.layers.Dense(50)(ins)\n",
    "x = keras.layers.ReLU()(x_)\n",
    "x = keras.layers.Dense(25)(x)\n",
    "x = keras.activations.exponential(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "#p = keras.layers.Dense(3)(x_)\n",
    "#p = keras.activations.softmax(p)\n",
    "\n",
    "model = keras.Model(inputs=ins, outputs=x)\n",
    "model.compile(loss=lossfn, optimizer=keras.optimizers.Adam()) # keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some messy pieces of Python magic, to make interactive training of a neural network a little\n",
    "# bit smoother. See the notes under Regression | Neural Network Model for how to use them.\n",
    "\n",
    "import IPython\n",
    "import signal\n",
    "\n",
    "def interrupted(_interrupted=[False], _default=[None]):\n",
    "    if _default[0] is None or signal.getsignal(signal.SIGINT) == _default[0]:\n",
    "        _interrupted[0] = False\n",
    "        def handle(signal, frame):\n",
    "            if _interrupted[0] and _default[0] is not None:\n",
    "                _default[0](signal, frame)\n",
    "            print('Interrupt!')\n",
    "            _interrupted[0] = True\n",
    "        _default[0] = signal.signal(signal.SIGINT, handle)\n",
    "    return _interrupted[0]\n",
    "\n",
    "def enumerate_cycle(g):\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        for i,x in enumerate(g):\n",
    "            yield (epoch,i), x\n",
    "        epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an endless iteration through epochs and batches of data\n",
    "BATCH_SIZE = 100\n",
    "indexes = np.arange(len(X))\n",
    "indexes = np.array_split(indexes, len(X) / BATCH_SIZE)\n",
    "eb_indexes = enumerate_cycle(indexes)\n",
    "\n",
    "# Accumulate some stats as we go along\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history) > 0 and loss_history[-1][0]-loss_history[0][0] >= 300):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indexes)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = model.train_on_batch(X[i], y1[i])\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indexes)}')\n",
    "        loss_history.append((time.time(), epoch+batch/len(indexes), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model._layers[-1]\n",
    "a.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y2[i][:5]\n",
    "print(keras.losses.binary_crossentropy(tf.constant(y2[i][:5]), (model.predict_on_batch(X[i][:5]))[-1]), '\\n')\n",
    "print(y2[i][:5], '\\n') \n",
    "print(model.predict_on_batch(X[i][:5]), '\\n')\n",
    "print(np.array(loss_history)[-5:, -1], model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot of the training loss\n",
    "df = pandas.DataFrame.from_records(loss_history, columns=['time','epoch','loss'])\n",
    "df.time = df.time - df.time[0]\n",
    "df['loss1'] = df.loss.map(lambda x: x[1])\n",
    "df['loss2'] = df.loss.map(lambda x: x[2])\n",
    "print(\"Final training loss:\", df.loss.ewm(com=5).mean().iloc[-1])\n",
    "fig,ax = plt.subplots(figsize=(6,2))\n",
    "#ax.plot(df.time, df.loss1)\n",
    "ax.plot(df.time, df.loss1.ewm(com=5).mean(), linewidth=1)\n",
    "ax.plot([0, 50],[-26.4, -26.4])\n",
    "#ax.plot(df.time, df.loss2-3)\n",
    "#ax.plot(df.time, df.loss2.ewm(com=10).mean()-3.6, linewidth=3)\n",
    "ax.set_xlim([0,45])\n",
    "plt.show()\n",
    "df.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[i][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca_res = pca.fit_transform(X)\n",
    "\n",
    "X_ = (pca_res[:, :70])\n",
    "X_ -= X_.min() - K.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insz = keras.layers.Input(shape=(70,))\n",
    "z = keras.layers.Dense(100)(insz)\n",
    "z = keras.activations.tanh(z)\n",
    "z = keras.layers.Dense(75)(z)\n",
    "z = keras.layers.ReLU()(z)\n",
    "z = keras.layers.Dense(50)(z)\n",
    "z = keras.layers.ReLU()(z)\n",
    "z = keras.layers.Dense(25)(z)\n",
    "z = keras.layers.ReLU()(z)\n",
    "z = keras.layers.Dense(1)(z)\n",
    "#x = keras.backend.argmax(x, dtype=float)\n",
    "\n",
    "modelz = keras.Model(inputs=insz, outputs=z)\n",
    "modelz.compile(loss=keras.losses.poisson, optimizer=keras.optimizers.Adam()) # keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history) > 0 and loss_history[-1][0]-loss_history[0][0] >= 60):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indexes)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = modelz.train_on_batch(X_[i], y[i])\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indexes)}' + f' time={loss_history[-1][0]-loss_history[0][0]}' if epoch > 0 else '')\n",
    "        loss_history.append((time.time(), epoch+batch/len(indexes), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelz.predict_on_batch(X_[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[i][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot of the training loss\n",
    "df = pandas.DataFrame.from_records(loss_history, columns=['time','epoch','loss'])\n",
    "df.time = df.time - df.time[0]\n",
    "print(\"Final training loss:\", df.loss.ewm(com=5).mean().iloc[-1])\n",
    "fig,ax = plt.subplots(figsize=(6,2))\n",
    "ax.plot(df.time, df.loss)\n",
    "ax.plot(df.time, df.loss.ewm(com=5).mean(), linewidth=3)\n",
    "ax.set_ylim([-3.5,0])\n",
    "plt.show()\n",
    "df.time[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50)\n",
    "tsne_res = tsne.fit_transform(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = tsne_res[:, 0], tsne_res[:, 1]\n",
    "\n",
    "plt.scatter(tx, ty)\n",
    "plt.savefig('tsne.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2 = TSNE(n_components=2, verbose=1)\n",
    "tsne2_res = tsne.fit_transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tx2, ty2 = tsne_res[:, 0], tsne_res[:, 1]\n",
    "fig, ax=plt.subplots()\n",
    "ax.scatter(tx2, ty2, s=0.2)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "np.random.seed(28)\n",
    "km = KMeans(n_clusters=8)\n",
    "km.fit(tsne_res)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "for inds, c, lab in zip([last_train, last_dev, last_val], ['#66c2a5', '#fc8d62', '#8da0cb'], ['Training', 'Development', 'Test']):\n",
    "    ax.scatter(tx2[inds], ty2[inds], s=0.5, c=c, label=lab)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "if False:\n",
    "    ax.spines['left'].set_color(\"white\")\n",
    "    ax.spines['bottom'].set_color(\"white\")\n",
    "else:\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "ax.set_aspect('equal')\n",
    "fig.tight_layout()\n",
    "fig.legend(frameon=False)    \n",
    "plt.savefig('figures/datasplit.pdf')\n",
    "    \n",
    "np.array([True, True, False]) & np.array([False, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = km.labels_ == 3\n",
    "val = km.labels_ == 4\n",
    "train = ~(dev | val)\n",
    "D = X[dev]\n",
    "V = X[val]\n",
    "T = X[train]\n",
    "ytrain = y[train]\n",
    "ydev = y[dev]\n",
    "yval = y[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all((~last_train | train)), last_train ^ train, last_train, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,p in enumerate(patient_batches):\n",
    "    if p[0].shape[0] != p[1].shape[0]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent stuff:\n",
    "# need to first separate data set into samples of patients. \n",
    "\n",
    "def get_patient(patient_nbr, samples=X, labels=y1):\n",
    "    inds = reduced_data.patient_nbr == patient_nbr\n",
    "    return samples[inds], labels[inds]\n",
    "\n",
    "patient_batches = [get_patient(nbr) for nbr in np.unique(reduced_data.patient_nbr.values)]\n",
    "\n",
    "ei_patients = enumerate_cycle(patient_batches)\n",
    "loss_history_rec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insrec = keras.layers.Input(shape=(None,121))\n",
    "xrec = keras.layers.GRU(units=150, return_sequences=True)(insrec)\n",
    "xrec = keras.layers.GRU(units=75, return_sequences=True)(xrec)\n",
    "xrec = keras.layers.Dense(40)(xrec)\n",
    "xrec = keras.layers.ReLU()(xrec)\n",
    "xrec = keras.layers.Dense(1)(xrec)\n",
    "modelrec = keras.models.Model(inputs=insrec, outputs=xrec)\n",
    "\n",
    "def recloss(y, y_): return keras.losses.poisson(y[0,-1], y_[:,-1,0])\n",
    "\n",
    "modelrec.compile(loss=lossfn, optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_patients = enumerate_cycle(train_batches)\n",
    "loss_history_rec = []\n",
    "\n",
    "while not interrupted():\n",
    "    if(len(loss_history_rec) > 0 and loss_history_rec[-1][0]-loss_history_rec[0][0] >= 300):\n",
    "        break\n",
    "    (epoch,item),patient = next(ei_patients)\n",
    "    loss = modelrec.train_on_batch(patient[0][np.newaxis,:,:], patient[1][np.newaxis, :, np.newaxis])\n",
    "    if item % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} item={item}/{len(patient_batches)}' + ( f' time={loss_history_rec[-1][0]-loss_history_rec[0][0]}' if len(loss_history_rec) > 0 else ''))\n",
    "        loss_history_rec.append((time.time(), epoch+item/len(patient_batches), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient[0][np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y__, y_t = modelrec.predict_on_batch(patient[0][np.newaxis,:,:]), patient[1][np.newaxis, :, np.newaxis]\n",
    "#recloss(y_t, y__)\n",
    "print(y_t.reshape(-1), y__.numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "a.extend([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Simple plot of the training loss\n",
    "df = pandas.DataFrame.from_records(loss_history_rec, columns=['time','epoch','loss'])\n",
    "df.time = df.time - df.time[0]\n",
    "print(\"Final training loss:\", df.loss.ewm(com=5).mean().iloc[-1])\n",
    "fig,ax = plt.subplots(figsize=(6,2))\n",
    "ax.plot(df.time, df.loss, alpha=0.2)\n",
    "ax.plot(df.time, df.loss.ewm(com=5).mean(), linewidth=1)\n",
    "ax.plot([0, 300], [-3,-3])\n",
    "#ax.set_ylim([-5,5])\n",
    "plt.show()\n",
    "df.time[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Simple plot of the training loss\n",
    "df = pandas.DataFrame.from_records(loss_history_rec, columns=['time','epoch','loss'])\n",
    "df.time = df.time - df.time[0]\n",
    "print(\"Final training loss:\", df.loss.ewm(com=5).mean().iloc[-1])\n",
    "fig,ax = plt.subplots(figsize=(6,2))\n",
    "ax.plot(df.time, df.loss, alpha=0.2)\n",
    "ax.plot(df.time, df.loss.ewm(com=5).mean(), linewidth=1)\n",
    "ax.plot([0, 300], [-26.4,-26.4])\n",
    "#ax.set_ylim([-5,5])\n",
    "plt.show()\n",
    "df.time[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "ys_ = []\n",
    "ys = []\n",
    "for p in tqdm(patient_batches):\n",
    "    ys_.extend(modelrec.predict_on_batch(p[0][np.newaxis,:,:]).numpy().reshape(-1))\n",
    "    ys.extend(p[1].reshape(-1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(ys_, bins=14*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysz_ = model.predict_on_batch(X)[0].numpy().reshape(-1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(y1, bins=14*2)\n",
    "ax.hist(ysz_, bins=14*5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysz_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_nbr = 88785891 #np.random.choice(reduced_data.patient_nbr.values)\n",
    "X_p, yp = get_patient(patient_nbr, samples=X)\n",
    "\n",
    "ypz_ = model.predict_on_batch(X_p)\n",
    "ypr_ = modelrec.predict_on_batch(X_p[np.newaxis,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypz_[0], ypr_, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ypz_[0].numpy().reshape(-1)[1:])\n",
    "#ax.plot(ypr_.numpy().reshape(-1))\n",
    "ax.plot(yp)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_on_batch(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.patient_nbr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_= val_rec_poiss\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for (y, y_) in tqdm(zip (recyval, ys_)):\n",
    "    if interrupted(): break\n",
    "    ax.plot([y-0.5, y+0.5], [y_, y_], linewidth=0.5, c='black')\n",
    "\n",
    "    ax.set_ylim([0.75, 14.25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recyval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypr_.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = keras.layers.Input(shape=(121,))\n",
    "t = keras.layers.Dense(100)(inst)\n",
    "t = keras.activations.tanh(t)\n",
    "t = keras.layers.Dense(75)(t)\n",
    "t = keras.layers.ReLU()(t)\n",
    "t = keras.layers.Dense(50)(t)\n",
    "t = keras.layers.ReLU()(t)\n",
    "t = keras.layers.Dense(50)(t)\n",
    "t = keras.layers.ReLU()(t)\n",
    "t = keras.layers.Dense(14)(t)\n",
    "t = keras.activations.softmax(t)\n",
    "\n",
    "modelt = keras.models.Model(inputs=inst, outputs=t)\n",
    "modelt.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycat = keras.utils.to_categorical(y1.reshape(-1,1))[:, 1:]\n",
    "ycat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "indices = np.arange(len(X))\n",
    "indices = np.array_split(indices, len(X) / BATCH_SIZE)\n",
    "eb_indices = enumerate_cycle(indices)\n",
    "\n",
    "# Accumulate some stats as we go along\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history) > 0 and loss_history[-1][0]-loss_history[0][0] >= 300):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indices)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = modelt.train_on_batch(X[i], ycat[i])\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}')\n",
    "        loss_history.append((time.time(), epoch+batch/len(indices), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.array([[1,4,5],[3,2,6]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycat.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yst_ = np.argmax(modelt.predict_on_batch(X).numpy(), axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots()\n",
    "ax.hist(yst_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelt.predict_on_batch(X).numpy()[:, np.array(y1, dtype=int)-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y1, dtype=int)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn(y1[i], model.predict_on_batch(X[i]).numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn(y1[i], model.predict_on_batch(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inss = keras.layers.Input(shape=(182,))\n",
    "xs = keras.layers.Dense(150)(inss)\n",
    "xs = keras.activations.tanh(xs)\n",
    "\n",
    "xs = keras.layers.Dense(150)(xs)\n",
    "xs = keras.layers.ReLU()(xs)\n",
    "\n",
    "xs = keras.layers.Dense(100)(xs)\n",
    "xs = keras.layers.ReLU()(xs)\n",
    "\n",
    "xs_ = keras.layers.Dense(75)(xs)\n",
    "xs = keras.layers.ReLU()(xs_)\n",
    "\n",
    "xs = keras.layers.Dense(40)(xs)\n",
    "xs = keras.activations.relu(xs)\n",
    "\n",
    "xs = keras.layers.Dense(1)(xs)\n",
    "xs = keras.layers.ReLU()(xs)\n",
    "#p = keras.layers.Dense(3)(x_)\n",
    "#p = keras.activations.softmax(p)\n",
    "\n",
    "smallmodel = keras.Model(inputs=inss, outputs=xs)\n",
    "smallmodel.compile(loss=keras.losses.poisson, optimizer=keras.optimizers.Adam()) # keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an endless iteration through epochs and batches of data\n",
    "BATCH_SIZE = 100\n",
    "indices = np.arange(len(T))\n",
    "indices = np.array_split(indices, len(T) / BATCH_SIZE)\n",
    "eb_indices = enumerate_cycle(indices)\n",
    "\n",
    "# Accumulate some stats as we go along\n",
    "loss_history_s = []\n",
    "dev_loss_history_small = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history_s) > 0 and loss_history_s[-1][0]-loss_history_s[0][0] >= 45):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indices)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = smallmodel.train_on_batch(T[i], ytrain[i])\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}' + f' time={loss_history_s[-1][0]-loss_history_s[0][0]}' if epoch > 0 else '')\n",
    "        loss_history_s.append((time.time(), epoch+batch/len(indices), loss))\n",
    "    if batch % 200 == 0:\n",
    "        devloss = tf.reduce_mean(keras.losses.poisson(ydev.reshape(-1,1), smallmodel.predict_on_batch(D)))\n",
    "        dev_loss_history_small.append((time.time(), epoch+batch/len(indices), devloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot of the training loss\n",
    "df = pandas.DataFrame.from_records(loss_history_s, columns=['time','epoch','loss'])\n",
    "df2 = pandas.DataFrame.from_records(dev_loss_history_small, columns=['time','epoch','loss'])\n",
    "df.time = df.time - df.time[0]\n",
    "df2.time = df2.time - df2.time[0]\n",
    "print(\"Final training loss:\", df.loss.ewm(com=5).mean().iloc[-1])\n",
    "fig,ax = plt.subplots(figsize=(6,2))\n",
    "ax.plot(df.time, df.loss, alpha=0.1)\n",
    "ax.plot(df.time, df.loss.ewm(com=5).mean(), linewidth=1)\n",
    "ax.plot(df2.time, df2.loss)\n",
    "#ax.set_ylim([-3.5,0])\n",
    "plt.show()\n",
    "df.time[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_rec_cat_prob[293]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yps_ = pca_cat_50.predict_on_batch(V_50).numpy()\n",
    "#ys_ = (yps_.argmax(axis=1) + 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for _y in np.arange(1,15):\n",
    "    inds = yval == _y\n",
    "    ys_rv = yps_[inds]\n",
    "    print(ys_rv.shape)\n",
    "    responsibilities = ys_rv.mean(axis=0)\n",
    "    print(responsibilities)\n",
    "    for i in np.arange(1,15):\n",
    "        ax.fill([_y-0.4875, _y-0.4875, _y+0.4875, _y+0.4875], [i-0.4875, i+0.4875, i+0.4875, i-0.4875], c='black', alpha=responsibilities[i-1])\n",
    "        \n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('white')\n",
    "ax.spines['bottom'].set_color('white')\n",
    "list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yps_ = med_model_cat.predict_on_batch(V).numpy()\n",
    "ys_ = (yps_.argmax(axis=1) + 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for _y in np.arange(1,15):\n",
    "    inds = yval == _y\n",
    "    ys_rv = yps_[inds]\n",
    "   # print(ys_rv.shape)\n",
    "    responsibilities = ys_rv.mean(axis=0)\n",
    "   # print(responsibilities)\n",
    "    for i in np.arange(1,15):\n",
    "        ax.fill([_y-0.5, _y-0.5, _y+0.5, _y+0.5], [i-0.5, i+0.5, i+0.5, i-0.5], c='black', alpha=responsibilities[i-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    ", \n",
    "yps_ = val_rec_poiss\n",
    "ys_ = med_model_poisson.predict_on_batch(D).numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for (y, y_) in tqdm(zip (yval_c, yps_)):\n",
    "    if interrupted(): break\n",
    "    ax.plot([y-0.5, y+0.5], [y_, y_], linewidth=0.5, c='black', alpha=200/(ydev==y).sum())\n",
    "\n",
    "ax.set_ylim([-0.5, 14.5])\n",
    "ax.set_xlim([-0.5, 14.5])\n",
    "    \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('white')\n",
    "ax.spines['bottom'].set_color('white')\n",
    "list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "plt.show()\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "for _y in np.arange(1,15):\n",
    "    inds = ytrain == _y\n",
    "    ys_rv = yps_[inds]\n",
    "   # print(ys_rv.shape)\n",
    "    responsibilities = ys_rv.mean(axis=0)\n",
    "   # print(responsibilities)\n",
    "    for i in np.arange(1,15):\n",
    "        ax.fill([_y-0.5, _y-0.5, _y+0.5, _y+0.5], [i-0.5, i+0.5, i+0.5, i-0.5], c='black', alpha=responsibilities[i-1])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insm = keras.layers.Input(shape=(121,))\n",
    "xm = keras.layers.Dense(175)(insm)\n",
    "xm = keras.activations.relu(xm)\n",
    "\n",
    "xm = keras.layers.Dense(225)(xm)\n",
    "xm = keras.activations.exponential(xm)\n",
    "\n",
    "xm = keras.layers.Dense(225)(xm)\n",
    "xm = keras.activations.relu(xm)\n",
    "\n",
    "xm = keras.layers.Dense(150)(xm)\n",
    "xm = keras.activations.relu(xm)\n",
    "\n",
    "xm = keras.layers.Dense(110)(xm)\n",
    "xm = keras.activations.relu(xm)\n",
    "\n",
    "xm_ = keras.layers.Dense(80)(xm)\n",
    "xm = keras.activations.relu(xm_)\n",
    "\n",
    "xm = keras.layers.Dense(50)(xm)\n",
    "xm = keras.activations.relu(xm)\n",
    "\n",
    "xm = keras.layers.Dense(1)(xm)\n",
    "xm = keras.layers.Lambda(lambda x: keras.activations.elu(x)+1)(xm)\n",
    "#p = keras.layers.Dense(3)(x_)\n",
    "#p = keras.activations.softmax(p)\n",
    "\n",
    "medmodel = keras.Model(inputs=insm, outputs=xm)\n",
    "medmodel.compile(loss=lossfn, optimizer=keras.optimizers.Adam()) # keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an endless iteration through epochs and batches of data\n",
    "BATCH_SIZE = 100\n",
    "indices = np.arange(len(T))\n",
    "indices = np.array_split(indices, len(T) / BATCH_SIZE)\n",
    "eb_indices = enumerate_cycle(indices)\n",
    "\n",
    "# Accumulate some stats as we go along\n",
    "loss_history_m = []\n",
    "dev_loss_history_med = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history_m) > 0 and loss_history_m[-1][0]-loss_history_m[0][0] >= 45):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indices)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = medmodel.train_on_batch(T[i], ytrain[i])\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}' + f' time={loss_history_m[-1][0]-loss_history_m[0][0]}' if epoch > 0 else '')\n",
    "        loss_history_m.append((time.time(), epoch+batch/len(indices), loss))\n",
    "    if batch % 200 == 0:\n",
    "        devloss = tf.reduce_mean(lossfn(ydev.reshape(-1,1), medmodel.predict_on_batch(D)))\n",
    "        dev_loss_history_med.append((time.time(), epoch+batch/len(indices), devloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot of the training loss\n",
    "dfs = pandas.DataFrame.from_records(loss_history_l, columns=['time','epoch','loss'])\n",
    "df2s = pandas.DataFrame.from_records(dev_loss_history_l, columns=['time','epoch','loss'])\n",
    "#df = pandas.DataFrame.from_records(loss_history_s, columns=['time','epoch','loss'])\n",
    "#df2 = pandas.DataFrame.from_records(dev_loss_history_small, columns=['time','epoch','loss'])\n",
    "#df.time = df.time - df.time[0]\n",
    "#df2.time = df2.time - df2.time[0]\n",
    "dfs.time = dfs.time - dfs.time[0]\n",
    "df2s.time = df2s.time - df2s.time[0]\n",
    "#print(\"Final training loss:\", df.loss.ewm(com=5).mean().iloc[-1])\n",
    "fig,ax = plt.subplots(figsize=(6,2))\n",
    "#ax.plot(df.time, df.loss, alpha=0.1)\n",
    "#ax.plot(df.time, df.loss.ewm(com=5).mean(), linewidth=1)\n",
    "#ax.plot(df2.time, df2.loss.ewm(com=5).mean())\n",
    "\n",
    "ax.plot(dfs.time, dfs.loss, alpha=0.1)\n",
    "ax.plot(dfs.time, dfs.loss.ewm(com=5).mean(), linewidth=1)\n",
    "ax.plot(df2s.time, df2s.loss.ewm(com=5).mean())\n",
    "ax.set_ylim([-1,3])\n",
    "plt.show()\n",
    "#df.time[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(keras.losses.poisson(ytrain[i].reshape(-1,1), lmodel.predict_on_batch(T[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (keras.layers.Input(1))\n",
    "a = keras.layers.Lambda(lambda x: keras.activations.elu(x)+1)(i)\n",
    "m = keras.models.Model(inputs=i, outputs=a)\n",
    "m.compile(loss=lambda x,y : 0.0, optimizer=keras.optimizers.Adam())\n",
    "m.predict_on_batch([-11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insl = keras.layers.Input(shape=(182,))\n",
    "xl = keras.layers.Dense(500)(insl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(400)(xl)\n",
    "xl = keras.activations.tanh(xl)\n",
    "\n",
    "xl = keras.layers.Dense(300)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(275)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(225)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(150)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(110)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl_ = keras.layers.Dense(80)(xl)\n",
    "xl = keras.activations.relu(xl_)\n",
    "\n",
    "xl = keras.layers.Dense(10)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(1)(xl)\n",
    "xl = keras.layers.Lambda(lambda x: keras.activations.elu(x-1, alpha=0.1)+1)(xl)\n",
    "\n",
    "lmodel = keras.Model(inputs=insl, outputs=xl)\n",
    "lmodel.compile(loss=keras.losses.poisson, optimizer=keras.optimizers.Adam()) # keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "indices = np.arange(len(T))\n",
    "indices = np.array_split(indices, len(T) / BATCH_SIZE)\n",
    "eb_indices = enumerate_cycle(indices)\n",
    "\n",
    "# Accumulate some stats as we go along\n",
    "loss_history_l = []\n",
    "dev_loss_history_l = []\n",
    "dev_loss_history_l_fs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history_l) > 0 and loss_history_l[-1][0]-loss_history_l[0][0] >= 45):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indices)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = lmodel.train_on_batch(T[i], ytrain[i])\n",
    "    if np.isnan(loss): break\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}' + f' time={loss_history_l[-1][0]-loss_history_l[0][0]}' if epoch > 0 else '')\n",
    "        loss_history_l.append((time.time(), epoch+batch/len(indices), loss))\n",
    "    if batch % 200 == 0:\n",
    "        devloss = lmodel.train_on_batch(D)\n",
    "        dev_loss_history_l.append((time.time(), epoch+batch/len(indices), devloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.predict_on_batch(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(T)+7*np.std(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, D, V = prepared_data2[train], prepared_data2[dev], prepared_data2[val] \n",
    "ytrain, ydev, yval = y1[train], y1[dev], y1[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda l: l[-1], loss_history_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insl = keras.layers.Input(shape=(182,))\n",
    "xl = keras.layers.Dense(500)(insl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(400)(xl)\n",
    "xl = keras.activations.tanh(xl)\n",
    "\n",
    "xl = keras.layers.Dense(300)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(275)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(225)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(150)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(110)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl_ = keras.layers.Dense(80)(xl)\n",
    "xl = keras.activations.relu(xl_)\n",
    "\n",
    "xl = keras.layers.Dense(40)(xl)\n",
    "xl = keras.activations.relu(xl)\n",
    "\n",
    "xl = keras.layers.Dense(14)(xl)\n",
    "xl = keras.activations.softmax(xl)\n",
    "\n",
    "lmodel = keras.Model(inputs=insl, outputs=xl)\n",
    "lmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam()) # keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob = np.repeat(np.arange(14).reshape(1,-1), len(y1), axis=0)\n",
    "yprob = yprob == np.repeat(y1-1, 14).reshape(-1,14)\n",
    "\n",
    "ytprob = np.repeat(np.arange(14).reshape(1,-1), len(ytrain), axis=0)\n",
    "ytprob[:, :] = ytprob == np.repeat(ytrain-1, 14).reshape(-1,14)\n",
    "ydprob = np.repeat(np.arange(14).reshape(1,-1), len(ydev), axis=0)\n",
    "ydprob[:, :] = ydprob == np.repeat(ydev-1, 14).reshape(-1,14)\n",
    "yvprob = np.repeat(np.arange(14).reshape(1,-1), len(yval), axis=0)\n",
    "yvprob[:, :] = yvprob == np.repeat(yval-1, 14).reshape(-1,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not interrupted():\n",
    "    if(len(loss_history_l) > 0 and loss_history_l[-1][0]-loss_history_l[0][0] >= 1800):\n",
    "        break\n",
    "    (epoch,batch),i = next(eb_indices)\n",
    "    #print(model.predict_on_batch(X[i]))\n",
    "    #input()\n",
    "    loss = lmodel.train_on_batch(T[i], ytprob[i])\n",
    "    if np.isnan(loss): break\n",
    "    if batch % 100 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}' + f' time={loss_history_l[-1][0]-loss_history_l[0][0]}' if epoch > 0 else '')\n",
    "        loss_history_l.append((time.time(), epoch+batch/len(indices), loss))\n",
    "    if batch % 200 == 0:\n",
    "        sample = np.random.choice(np.arange(len(D)), 10*BATCH_SIZE, replace=False)\n",
    "        devloss = tf.reduce_mean(keras.losses.categorical_crossentropy(ydprob[sample], lmodel.predict_on_batch(D[sample])))\n",
    "        dev_loss_history_l.append((time.time(), epoch+batch/len(indices), devloss))\n",
    "        if len(dev_loss_history_l) > 20 and dev_loss_history_l[-1][-1] > dev_loss_history_l[-2][-1] and dev_loss_history_l[-2][-1] > dev_loss_history_l[-3][-1] and ((dev_loss_history_l[-1][-1] > dev_loss_history_l[-3][-1] + 0.075) or (dev_loss_history_l[-1][-1] > min(map(lambda x: x[-1], dev_loss_history_l)) + 0.1)):\n",
    "            print(\"FAAIIIIL\", list(map(lambda x: x[-1], dev_loss_history_l)))\n",
    "            dev_loss_history_l_fs.append((time.time(), epoch+batch/len(indices), devloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loss_history_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmodel.train_on_batch(T[i], ytprob[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.3079793 -  2.2988513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,4,3], [4,8,6], [7,12,9]]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "cs = np.random.random(14)\n",
    "cs /= cs.sum()\n",
    "\n",
    "for i in np.arange(1,15):\n",
    "    ax.fill([0,1,1,0], [i-1, i, i, i-1], c='black', alpha=cs[i-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL MEASUREMENTS...\n",
    "\n",
    "## MLP\n",
    "\n",
    "Divide as follows:\n",
    "\n",
    " - poisson loss\n",
    "   - small network\n",
    "   - med network\n",
    "   - big network\n",
    " - cat loss\n",
    "   - small ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "ins = keras.layers.Input(182)\n",
    "poisson = keras.losses.poisson\n",
    "cat = keras.losses.categorical_crossentropy\n",
    "Adam = keras.optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small model\n",
    "def regenerate_small():\n",
    "    x_small = keras.layers.Dense(150)(ins)\n",
    "    x_small = keras.layers.ReLU()(x_small)\n",
    "\n",
    "    x_small = keras.layers.Dense(150)(x_small)\n",
    "    x_small = keras.layers.ReLU()(x_small)\n",
    "\n",
    "    x_small = keras.layers.Dense(100)(x_small)\n",
    "    x_small = keras.layers.ReLU()(x_small)\n",
    "\n",
    "    x_small = keras.layers.Dense(75)(x_small)\n",
    "    x_small = keras.layers.ReLU()(x_small)\n",
    "\n",
    "    x_small = keras.layers.Dense(40)(x_small)\n",
    "    x_small = keras.activations.relu(x_small)\n",
    "\n",
    "    poisson_out_small = keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2())(x_small)\n",
    "    poisson_out_small = keras.layers.ReLU()(poisson_out_small)\n",
    "\n",
    "    cat_out_small = keras.layers.Dense(14)(x_small)\n",
    "    cat_out_small = keras.activations.softmax(cat_out_small)\n",
    "    \n",
    "    small_model_poisson = keras.models.Model(inputs=ins, outputs=poisson_out_small)\n",
    "    small_model_poisson.compile(loss=poisson, optimizer=Adam())\n",
    "\n",
    "    small_model_cat = keras.models.Model(inputs=ins, outputs=cat_out_small)\n",
    "    small_model_cat.compile(loss=cat, optimizer=Adam())\n",
    "    \n",
    "    return small_model_poisson, small_model_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_poisson = keras.models.Model(inputs=ins, outputs=poisson_out_small)\n",
    "small_model_poisson.compile(loss=poisson, optimizer=Adam())\n",
    "\n",
    "small_model_cat = keras.models.Model(inputs=ins, outputs=cat_out_small)\n",
    "small_model_cat.compile(loss=cat, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an endless iteration through epochs and batches of data\n",
    "\n",
    "def experiment(model, lossmthd, trainy, devy, maxtime):\n",
    "    BATCH_SIZE = 100\n",
    "    indices = np.arange(len(T))\n",
    "    indices = np.array_split(indices, len(T) / BATCH_SIZE)\n",
    "    eb_indices = enumerate_cycle(indices)\n",
    "\n",
    "    # Accumulate some stats as we go along\n",
    "    loss_history_l = []\n",
    "    dev_loss_history_l = []\n",
    "    dev_loss_history_l_fs = []\n",
    "\n",
    "    model.compile(loss=lossmthd, optimizer=Adam())\n",
    "    \n",
    "    while not interrupted():\n",
    "\n",
    "        \n",
    "        if(len(loss_history_l) > 0 and loss_history_l[-1][0]-loss_history_l[0][0] >= maxtime):\n",
    "            break\n",
    "        (epoch,batch),i = next(eb_indices)\n",
    "        #print(model.predict_on_batch(X[i]))\n",
    "        #input()\n",
    "        loss = model.train_on_batch(T[i], trainy[i])\n",
    "        if np.isnan(loss): break\n",
    "        if batch % 100 == 0:\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}' + f' time={loss_history_l[-1][0]-loss_history_l[0][0]}' if epoch > 0 else '')\n",
    "            loss_history_l.append((time.time(), epoch+batch/len(indices), loss))\n",
    "            sample = np.random.choice(np.arange(len(D)), 4*BATCH_SIZE, replace=False)\n",
    "            print(ydprob[sample].shape,  model.predict_on_batch(D[sample]).numpy().shape)\n",
    "            devloss = tf.reduce_mean(lossmthd(devy[sample], model.predict_on_batch(D[sample])))\n",
    "            dev_loss_history_l.append((time.time(), epoch+batch/len(indices), devloss))\n",
    "            if len(dev_loss_history_l) > 20 and dev_loss_history_l[-1][-1] > dev_loss_history_l[-2][-1] and dev_loss_history_l[-2][-1] > dev_loss_history_l[-3][-1] and ((dev_loss_history_l[-1][-1] > dev_loss_history_l[-3][-1] + 0.075) or (dev_loss_history_l[-1][-1] > min(map(lambda x: x[-1], dev_loss_history_l)) + 0.1)):\n",
    "                print(\"FAAIIIIL\")\n",
    "                dev_loss_history_l_fs.append((time.time(), epoch+batch/len(indices), devloss))\n",
    "            else:\n",
    "                print()\n",
    "                                  \n",
    "    return loss_history_l, dev_loss_history_l, dev_loss_history_l_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_poisson, small_model_cat = regenerate_small()\n",
    "losses_small_poisson, dev_losses_small_poisson, fails_small_poisson = experiment(small_model_poisson, poisson, ytrain, ydev, 120)\n",
    "small_model_poisson, small_model_cat = regenerate_small()\n",
    "losses_small_cat, dev_losses_small_cat, fails_small_cat = experiment(small_model_cat, cat, ytprob, ydprob, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_losses(losses, dev_losses, fails, filename, yaxis=False, comp=10):\n",
    "    devdf = pandas.DataFrame.from_records(losses, columns=['time','epoch','loss'])\n",
    "    df2s = pandas.DataFrame.from_records(dev_losses, columns=['time','epoch','loss'])\n",
    "    devdf.time = devdf.time - devdf.time[0]\n",
    "    df2s.time = df2s.time - df2s.time[0]\n",
    "    fig,ax = plt.subplots(figsize=(8,4))\n",
    "    \n",
    "    t1 = devdf[devdf.epoch == 1].time.values[0]\n",
    "    t2 = devdf[devdf.epoch == 1].time.values[-1]\n",
    "    \n",
    "    ax.axvspan(t1, t2, color='gray', alpha=0.07)\n",
    "\n",
    "    ax.plot(devdf.time, devdf.loss.values, alpha=0.1, c='black')\n",
    "    ax.plot(devdf.time, devdf.loss.ewm(com=comp).mean(), linewidth=1, c='black', label='Training Loss')\n",
    "    ax.plot(df2s.time, df2s.loss.ewm(com=comp).mean(), c='#66c2a5', label='Development Loss')\n",
    "    \n",
    "   # for i,f in enumerate(fails):\n",
    "   #     t, b, l = f\n",
    "   #     ax.plot([t-losses[0][0], t-losses[0][0]], [-100, 100], c='#fc8d62', linewidth=0.5)\n",
    "    upper_lim = devdf.loss.append(df2s.loss).values.reshape(-1).argsort()[int(0.975*(len(devdf) + len(df2s)))]\n",
    "    upper_lim = float(devdf.loss.append(df2s.loss).values[upper_lim])\n",
    "    lower_lim = devdf.loss.append(df2s.loss).values.reshape(-1).argsort()[int(0.01*(len(devdf) + len(df2s)))]\n",
    "    lower_lim = float(devdf.loss.append(df2s.loss).values[lower_lim])\n",
    "    print(upper_lim)\n",
    "    ax.set_ylim([lower_lim, upper_lim + 0.1*abs(upper_lim)])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    if yaxis:\n",
    "        ax.spines['left'].set_color(\"white\")\n",
    "        ax.spines['bottom'].set_color(\"white\")\n",
    "    else:\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "    ax.set_xlabel('training time in seconds')\n",
    "    if yaxis:\n",
    "        ax.set_ylabel(r'$\\ln\\mathrm{lik}(\\theta | \\mathbf{y}) - c$', rotation=0, horizontalalignment='right')\n",
    "    fig.tight_layout()\n",
    "    plt.legend(frameon=False)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(devdf.time.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_losses(losses_small_poisson, dev_losses_small_poisson, fails_small_poisson, 'figures/basic_small_poisson.pdf', yaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, small_model_cat = regenerate_small()\n",
    "losses_small_cat, dev_losses_small_cat, _ = experiment(small_model_cat, cat, ytprob, ydprob, 120)\n",
    "draw_losses(losses_small_cat, dev_losses_small_cat, fails_small_poisson, 'figures/basic_small_cat.pdf', yaxis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_losses(losses_small_cat, dev_losses_small_cat, fails_small_poisson, 'figures/basic_small_cat.pdf', yaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_medium():\n",
    "    x_med = keras.layers.Dense(200)(ins)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    \n",
    "    x_med = keras.layers.Dense(225)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(185)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(150)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(110)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(85)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(50)(x_med)\n",
    "    x_med = keras.activations.relu(x_med)\n",
    "\n",
    "    poisson_out_med = keras.layers.Dense(1)(x_med)\n",
    "    poisson_out_med = keras.layers.ReLU()(poisson_out_med)\n",
    "\n",
    "    cat_out_med = keras.layers.Dense(14)(x_med)\n",
    "    cat_out_med = keras.activations.softmax(cat_out_med)\n",
    "    \n",
    "    med_model_poisson = keras.models.Model(inputs=ins, outputs=poisson_out_med)\n",
    "    med_model_poisson.compile(loss=poisson, optimizer=Adam())\n",
    "\n",
    "    med_model_cat = keras.models.Model(inputs=ins, outputs=cat_out_med)\n",
    "    med_model_cat.compile(loss=cat, optimizer=Adam())\n",
    "    \n",
    "    return med_model_poisson, med_model_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_model_poisson, _ = regenerate_medium()\n",
    "\n",
    "losses_med_poisson, dev_losses_med_poisson, f = experiment(med_model_poisson, poisson,ytrain, ydev, 120)\n",
    "\n",
    "med_model_poisson, med_model_cat = regenerate_medium()\n",
    "losses_med_cat, dev_losses_med_cat, f = experiment(med_model_cat, cat, ytprob, ydprob, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_losses(losses_med_poisson, dev_losses_med_poisson, f, 'figures/basic_med_poisson.pdf')\n",
    "draw_losses(losses_med_cat, dev_losses_med_cat, f, 'figures/basic_med_cat.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_large():\n",
    "    x_med = keras.layers.Dense(375)(ins)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    \n",
    "    x_med = keras.layers.Dense(450)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(450)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(325)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(275)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(130)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(115)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(90)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(75)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(50)(x_med)\n",
    "    x_med = keras.activations.relu(x_med)\n",
    "\n",
    "    poisson_out_med = keras.layers.Dense(1)(x_med)\n",
    "    poisson_out_med = keras.layers.ReLU()(poisson_out_med)\n",
    "\n",
    "    cat_out_med = keras.layers.Dense(14)(x_med)\n",
    "    cat_out_med = keras.activations.softmax(cat_out_med)\n",
    "    \n",
    "    med_model_poisson = keras.models.Model(inputs=ins, outputs=poisson_out_med)\n",
    "    med_model_poisson.compile(loss=poisson, optimizer=Adam())\n",
    "\n",
    "    med_model_cat = keras.models.Model(inputs=ins, outputs=cat_out_med)\n",
    "    med_model_cat.compile(loss=cat, optimizer=Adam())\n",
    "    \n",
    "    return med_model_poisson, med_model_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model_poisson, _ = regenerate_large()\n",
    "\n",
    "losses_large_poisson, dev_losses_large_poisson, f = experiment(large_model_poisson, poisson, ytrain, ydev, 120)\n",
    "                                                              \n",
    "_, large_model_cat = regenerate_large()                                                              \n",
    "losses_large_cat, dev_losses_large_cat, f = experiment(large_model_cat, cat, ytprob, ydprob, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_losses(losses_large_poisson, dev_losses_large_poisson, f, 'figures/basic_large_poisson.pdf')\n",
    "draw_losses(losses_large_cat, dev_losses_large_cat, f, 'figures/basic_large_cat.pdf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def draw_comparison(small, med, large, filename, yaxis=False, legend=True, comp=10):\n",
    "    dfsmall = pandas.DataFrame.from_records(small, columns=['time','epoch','loss'])\n",
    "    dfmed = pandas.DataFrame.from_records(med, columns=['time','epoch','loss'])\n",
    "    dflrg = pandas.DataFrame.from_records(large, columns=['time','epoch','loss'])\n",
    "    dfsmall.time = dfsmall.time - dfsmall.time[0]\n",
    "    dfmed.time = dfmed.time - dfmed.time[0]\n",
    "    dflrg.time = dflrg.time - dflrg.time[0]\n",
    "    fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    ax.plot(dfsmall.time, dfsmall.loss.ewm(com=comp).mean(), linewidth=0.75, c='#a6cee3', label='Small Net')\n",
    "    ax.plot(dfmed.time, dfmed.loss.ewm(com=comp).mean(), linewidth=0.75, c='#1f78b4', label='Medium Net')\n",
    "    ax.plot(dflrg.time, dflrg.loss.ewm(com=comp).mean(), linewidth=0.75, c='#b2df8a', label='Large Net')\n",
    "    \n",
    "   # for i,f in enumerate(fails):\n",
    "   #     t, b, l = f\n",
    "   #     ax.plot([t-losses[0][0], t-losses[0][0]], [-100, 100], c='#fc8d62', linewidth=0.5)\n",
    "    upper_lim = dfsmall.loss.append(dfmed.loss).values.reshape(-1).argsort()[int(0.975*(len(dfsmall) + len(dfmed)))]\n",
    "    upper_lim = float(dfsmall.loss.append(dfmed.loss).values[upper_lim])\n",
    "    lower_lim = dfsmall.loss.append(dfmed.loss).values.reshape(-1).argsort()[int(0.005*(len(dfsmall) + len(dfmed)))]\n",
    "    lower_lim = float(dfsmall.loss.append(dfmed.loss).values[lower_lim])\n",
    "    #lower_lim = float(dfsmall.append(dfmed).append(dflrg).loss.min() - 0.1*abs(dfsmall.append(dfmed).append(dflrg).loss.min()))\n",
    "    print(upper_lim)\n",
    "    ax.set_ylim([lower_lim, upper_lim + 0.1*abs(upper_lim)])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    if yaxis:\n",
    "        ax.spines['left'].set_color(\"white\")\n",
    "        ax.spines['bottom'].set_color(\"white\")\n",
    "    else:\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "    ax.set_xlabel('training time in seconds')\n",
    "    if yaxis:\n",
    "        ax.set_ylabel(r'$\\ln\\mathrm{lik}(\\theta | \\mathbf{y}) - c$', rotation=0, horizontalalignment='right')\n",
    "    if legend: plt.legend(frameon=False)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(dfsmall.time.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_comparison(losses_small_poisson, losses_med_poisson, losses_large_poisson, 'figures/comparison_poisson_mlp.pdf', yaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_comparison(losses_small_cat, losses_med_cat, losses_large_cat, 'figures/comparison_cat_mlp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_pca(dim):\n",
    "    ins = keras.layers.Input(dim)\n",
    "    \n",
    "    x_med = keras.layers.Dense(375)(ins)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    \n",
    "    x_med = keras.layers.Dense(450, kernel_regularizer=keras.regularizers.l2(0.05))(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(450)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(325, kernel_regularizer=keras.regularizers.l2(0.1))(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(275)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(130)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(115, kernel_regularizer=keras.regularizers.l2(0.05))(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "    \n",
    "    x_med = keras.layers.Dense(90)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(75)(x_med)\n",
    "    x_med = keras.layers.ReLU()(x_med)\n",
    "\n",
    "    x_med = keras.layers.Dense(50)(x_med)\n",
    "    x_med = keras.activations.relu(x_med)\n",
    "\n",
    "    poisson_out_med = keras.layers.Dense(1)(x_med)\n",
    "    poisson_out_med = keras.layers.ReLU()(poisson_out_med)\n",
    "\n",
    "    cat_out_med = keras.layers.Dense(14)(x_med)\n",
    "    cat_out_med = keras.activations.softmax(cat_out_med)\n",
    "    \n",
    "    med_model_poisson = keras.models.Model(inputs=ins, outputs=poisson_out_med)\n",
    "    med_model_poisson.compile(loss=poisson, optimizer=Adam())\n",
    "\n",
    "    med_model_cat = keras.models.Model(inputs=ins, outputs=cat_out_med)\n",
    "    med_model_cat.compile(loss=cat, optimizer=Adam())\n",
    "    \n",
    "    return med_model_poisson, med_model_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca_res = pca.fit_transform(T)\n",
    "\n",
    "T_50, T_85, T_125 = pca_res[:, :50], pca_res[:, :85], pca_res[:, :125]\n",
    "\n",
    "DPCA, VPCA = pca.transform(D), pca.transform(V)\n",
    "\n",
    "D_50, D_85, D_125 = DPCA[:, :50], DPCA[:, :85], DPCA[:, :125]\n",
    "V_50, V_85, V_125 = VPCA[:, :50], VPCA[:, :85], VPCA[:, :125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_poiss_50, pca_cat_50 = regenerate_pca(50)\n",
    "pca_poiss_85 , pca_cat_85 = regenerate_pca(85)\n",
    "pca_poiss_125, pca_cat_125 = regenerate_pca(125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_experiment(model, lossmthd, trainx, trainy, devx, devy, maxtime):\n",
    "    BATCH_SIZE = 100\n",
    "    indices = np.arange(len(trainx))\n",
    "    indices = np.array_split(indices, len(trainx) / BATCH_SIZE)\n",
    "    eb_indices = enumerate_cycle(indices)\n",
    "\n",
    "    # Accumulate some stats as we go along\n",
    "    loss_history_l = []\n",
    "    dev_loss_history_l = []\n",
    "    dev_loss_history_l_fs = []\n",
    "\n",
    "    model.compile(loss=lossmthd, optimizer=Adam())\n",
    "    while not interrupted():\n",
    "        if(len(loss_history_l) > 0 and loss_history_l[-1][0]-loss_history_l[0][0] >= maxtime):\n",
    "            break\n",
    "        (epoch,batch),i = next(eb_indices)\n",
    "        #print(model.predict_on_batch(X[i]))\n",
    "        #input()\n",
    "        loss = model.train_on_batch(trainx[i], trainy[i])\n",
    "        if np.isnan(loss): break\n",
    "        if batch % 100 == 0:\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            print(f'loss={loss} epoch={epoch} batchnum={batch}/{len(indices)}' + f' time={loss_history_l[-1][0]-loss_history_l[0][0]}' if epoch > 0 else '')\n",
    "            loss_history_l.append((time.time(), epoch+batch/len(indices), loss))\n",
    "            sample = np.random.choice(np.arange(len(devx)), 4*BATCH_SIZE, replace=False)\n",
    "            print(ydprob[sample].shape,  model.predict_on_batch(devx[sample]).numpy().shape)\n",
    "            devloss = tf.reduce_mean(lossmthd(devy[sample], model.predict_on_batch(devx[sample])))\n",
    "            dev_loss_history_l.append((time.time(), epoch+batch/len(indices), devloss))\n",
    "            if len(dev_loss_history_l) > 20 and dev_loss_history_l[-1][-1] > dev_loss_history_l[-2][-1] and dev_loss_history_l[-2][-1] > dev_loss_history_l[-3][-1] and ((dev_loss_history_l[-1][-1] > dev_loss_history_l[-3][-1] + 0.075) or (dev_loss_history_l[-1][-1] > min(map(lambda x: x[-1], dev_loss_history_l)) + 0.1)):\n",
    "                print(\"FAAIIIIL\")\n",
    "                dev_loss_history_l_fs.append((time.time(), epoch+batch/len(indices), devloss))\n",
    "            else:\n",
    "                print()\n",
    "                                  \n",
    "    return loss_history_l, dev_loss_history_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_poiss_50, pca_cat_50 = regenerate_pca( 50)    ; pca_50_poiss_loss, pca_50_poiss_devloss = pca_experiment(pca_poiss_50, poisson, T_50, ytrain, D_50, ydev, 120)\n",
    "pca_poiss_85, pca_cat_85 = regenerate_pca( 85)    ; pca_85_poiss_loss, pca_85_poiss_devloss = pca_experiment(pca_poiss_85, poisson, T_85, ytrain, D_85, ydev, 120)\n",
    "pca_poiss_125, pca_cat_125 = regenerate_pca(125)  ; pca_125_poiss_loss, pca_125_poiss_devloss = pca_experiment(pca_poiss_125, poisson, T_125, ytrain, D_125, ydev, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_poiss_50, pca_cat_50 = regenerate_pca( 50)    ;pca_50_cat_loss, pca_50_cat_devloss = pca_experiment(pca_cat_50, cat, T_50, ytprob, D_50, ydprob, 120)\n",
    "pca_poiss_85, pca_cat_85 = regenerate_pca( 85)    ;pca_85_cat_loss, pca_85_cat_devloss = pca_experiment(pca_cat_85, cat, T_85, ytprob, D_85, ydprob, 120)\n",
    "pca_poiss_125, pca_cat_125 = regenerate_pca(125)  ;pca_125_cat_loss, pca_125_cat_devloss = pca_experiment(pca_cat_125, cat, T_125, ytprob, D_125, ydprob, 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pca_losses(losses, dev_losses, fails, filename, yaxis=False):\n",
    "    devdf = pandas.DataFrame.from_records(losses, columns=['time','epoch','loss'])\n",
    "    df2s = pandas.DataFrame.from_records(dev_losses, columns=['time','epoch','loss'])\n",
    "    devdf.time = devdf.time - devdf.time[0]\n",
    "    df2s.time = df2s.time - df2s.time[0]\n",
    "    fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    ax.plot(devdf.time, devdf.loss, alpha=0.1, c='black')\n",
    "    ax.plot(devdf.time, devdf.loss.ewm(com=10).mean(), linewidth=1, c='black')\n",
    "    ax.plot(df2s.time, df2s.loss.ewm(com=10).mean(), c='#66c2a5')\n",
    "    \n",
    "   # for i,f in enumerate(fails):\n",
    "   #     t, b, l = f\n",
    "   #     ax.plot([t-losses[0][0], t-losses[0][0]], [-100, 100], c='#fc8d62', linewidth=0.5)\n",
    "    upper_lim = devdf.loss.append(df2s.loss).values.reshape(-1).argsort()[int(0.975*(len(devdf) + len(df2s)))]\n",
    "    upper_lim = float(devdf.loss.append(df2s.loss).values[upper_lim])\n",
    "    print(upper_lim)\n",
    "    ax.set_ylim([devdf.loss.values.min() - 0.1*abs(devdf.loss.values.min()), upper_lim + 0.1*abs(upper_lim)])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    if yaxis:\n",
    "        ax.spines['left'].set_color(\"white\")\n",
    "        ax.spines['bottom'].set_color(\"white\")\n",
    "    else:\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "    ax.set_xlabel('training time in seconds')\n",
    "    if yaxis:\n",
    "        ax.set_ylabel(r'$\\ln\\mathrm{lik}(\\theta | \\mathbf{y}) - c$', rotation=0, horizontalalignment='right')\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(devdf.time.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pca_losses(pca_50_poiss_loss, pca_50_poiss_devloss, [], 'figures/pca_50_poiss.pdf', yaxis=True)\n",
    "draw_pca_losses(pca_85_poiss_loss, pca_85_poiss_devloss, [], 'figures/pca_85_poiss.pdf')\n",
    "draw_pca_losses(pca_125_poiss_loss, pca_125_poiss_devloss, [], 'figures/pca_125_poiss.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pca_losses(pca_50_cat_loss, pca_50_cat_devloss, [], 'figures/pca_50_cat.pdf', yaxis=True)\n",
    "draw_pca_losses(pca_85_cat_loss, pca_85_cat_devloss, [], 'figures/pca_85_cat.pdf')\n",
    "draw_pca_losses(pca_125_cat_loss, pca_125_cat_devloss, [], 'figures/pca_125_cat.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pca_comparison(small, med, large, old, filename, yaxis=False, legend=True):\n",
    "    dfsmall = pandas.DataFrame.from_records(small, columns=['time','epoch','loss'])\n",
    "    dfmed = pandas.DataFrame.from_records(med, columns=['time','epoch','loss'])\n",
    "    dflrg = pandas.DataFrame.from_records(large, columns=['time','epoch','loss'])\n",
    "    dfold = pandas.DataFrame.from_records(old, columns=['time','epoch','loss'])\n",
    "    dfsmall.time = dfsmall.time - dfsmall.time[0]\n",
    "    dfmed.time = dfmed.time - dfmed.time[0]\n",
    "    dflrg.time = dflrg.time - dflrg.time[0]\n",
    "    dfold.time = dfold.time - dfold.time[0] if len(dfold) > 0 else dfold.time\n",
    "    fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    ax.plot(dfsmall.time, dfsmall.loss.ewm(com=10).mean(), linewidth=0.75, c='#a6cee3', label='50 Dimensions')\n",
    "    ax.plot(dfmed.time, dfmed.loss.ewm(com=10).mean(), linewidth=0.75, c='#1f78b4', label='85 Dimensions')\n",
    "    ax.plot(dflrg.time, dflrg.loss.ewm(com=10).mean(), linewidth=0.75, c='#b2df8a', label='125 Dimensions')\n",
    "    if old != []:\n",
    "        ax.plot(dfold.time, dfold.loss.ewm(com=10).mean(), linewidth=0.75, c='black', label='Unreduced Data,\\nNo regularization.')\n",
    "    \n",
    "   # for i,f in enumerate(fails):\n",
    "   #     t, b, l = f\n",
    "   #     ax.plot([t-losses[0][0], t-losses[0][0]], [-100, 100], c='#fc8d62', linewidth=0.5)\n",
    "    upper_lim = dfsmall.loss.append(dfmed.loss).append(dflrg.loss).append(dfold.loss).values.reshape(-1).argsort()[int(0.975*(len(dfsmall) + len(dfmed) + len(dfold.append(dflrg))))]\n",
    "    upper_lim = float(dfsmall.loss.append(dfmed.loss).append(dflrg.loss).append(dfold.loss).values[upper_lim])\n",
    "    lower_lim = dfsmall.append(dfmed).append(dflrg).append(dfold).loss.min() - 0.05*abs(dfsmall.append(dfmed).append(dflrg).append(dfold).loss.min())\n",
    "    lower_lim = float(lower_lim)\n",
    "    print(lower_lim)\n",
    "    ax.set_ylim([lower_lim, upper_lim + 0.1*abs(upper_lim)])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    if yaxis:\n",
    "        ax.spines['left'].set_color(\"white\")\n",
    "        ax.spines['bottom'].set_color(\"white\")\n",
    "    else:\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "    list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "    ax.set_xlabel('training time in seconds')\n",
    "    if yaxis:\n",
    "        ax.set_ylabel(r'$\\ln\\mathrm{lik}(\\theta | \\mathbf{y}) - c$', rotation=0, horizontalalignment='right')\n",
    "    if legend: plt.legend(frameon=False)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(dfsmall.time.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pca_comparison(pca_50_poiss_loss, pca_85_poiss_loss, pca_125_poiss_loss, losses_large_poisson, 'figures/comparison_poiss_pca2.pdf', yaxis=True)\n",
    "draw_pca_comparison(pca_50_poiss_devloss, pca_85_poiss_devloss, pca_125_poiss_devloss, dev_losses_large_poisson, 'figures/comparison_poiss_loss_pca.pdf', yaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pca_comparison(pca_50_cat_loss, pca_85_cat_loss, pca_125_cat_loss, [], 'figures/comparison_cat_pca.pdf')\n",
    "draw_pca_comparison(pca_50_cat_devloss, pca_85_cat_devloss, pca_125_cat_devloss, [], 'figures/comparison_cat_devloss_pca.pdf', yaxis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pca_comparison(pca_50_poiss_devloss, pca_85_poiss_devloss, pca_125_poiss_devloss, dev_losses_med_poisson, 'figures/comparison_poiss_loss_pca.pdf', yaxis=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pca_comparison(pca_50_cat_devloss, pca_85_cat_devloss, pca_125_cat_devloss, [], 'figures/comparison_cat_loss_pca.pdf', yaxis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_comparison(dev_losses_small_cat, dev_losses_med_cat, dev_losses_large_cat, 'figures/devlosscomp_mlp_cat.pdf')\n",
    "\n",
    "draw_comparison(dev_losses_small_poisson, dev_losses_med_poisson, dev_losses_large_poisson, 'figures/devlosscomp_mlp_poiss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training, dev and val sets for recurrent structure.\n",
    "\n",
    "last_indices = reduced_data.groupby(['patient_nbr']).apply(lambda x: x.encounter_id.values.max())\n",
    "last_indices = reduced_data.encounter_id.map(lambda i : i in last_indices.values)\n",
    "last_val = last_indices.values & val\n",
    "last_dev = last_indices.values & dev\n",
    "last_train = last_indices.values & train\n",
    "\n",
    "#last_train.shape\n",
    "\n",
    "train_patients = reduced_data.patient_nbr[last_train]\n",
    "dev_patients = reduced_data.patient_nbr[last_dev]\n",
    "val_patients = reduced_data.patient_nbr[last_val]\n",
    "\n",
    "X_ = pca.transform(prepared_data2)[:, :85]\n",
    "\n",
    "train_batches_poisson = np.array([get_patient(pn, samples=X_, labels=y1) for pn in train_patients])\n",
    "dev_batches_poisson = np.array([get_patient(pn, samples=X_, labels=y1) for pn in dev_patients])\n",
    "val_batches_poisson = np.array([get_patient(pn, samples=X_, labels=y1) for pn in val_patients])\n",
    "\n",
    "train_batches_cat = np.array([get_patient(pn, samples=X_, labels=yprob) for pn in train_patients])\n",
    "dev_batches_cat = np.array([get_patient(pn, samples=X_, labels=yprob) for pn in dev_patients])\n",
    "val_batches_cat = np.array([get_patient(pn, samples=X_, labels=yprob) for pn in val_patients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_cat[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_poisson[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_rec():\n",
    "    insrec = keras.layers.Input(shape=(None,85))\n",
    "    xrec = keras.layers.GRU(units=400, return_sequences=True)(insrec)\n",
    "    xrec = keras.layers.GRU(units=450, return_sequences=True)(xrec)\n",
    "    xrec = keras.layers.GRU(units=400, return_sequences=True)(xrec)\n",
    "    xrec = keras.layers.Dense(300)(xrec)\n",
    "    xrec = keras.layers.ReLU()(xrec)\n",
    "    xrec = keras.layers.Dense(200)(xrec)\n",
    "    xrec = keras.layers.ReLU()(xrec)\n",
    "    xrec = keras.layers.Dense(100)(xrec)\n",
    "    xrec = keras.layers.ReLU()(xrec)\n",
    "    xrec = keras.layers.Dense(50)(xrec)\n",
    "    xrec = keras.layers.ReLU()(xrec)\n",
    "    \n",
    "    xpoiss = keras.layers.Dense(1)(xrec)\n",
    "    xpoiss = keras.layers.ReLU()(xpoiss)\n",
    "    \n",
    "    xcat = keras.layers.Dense(14)(xrec)\n",
    "    xcat = keras.activations.softmax(xcat)\n",
    "    \n",
    "    poisson_model = keras.models.Model(inputs=insrec, outputs=xpoiss)\n",
    "    poisson_model.compile(loss=rec_poiss, optimizer=Adam())\n",
    "    \n",
    "    cat_model = keras.models.Model(inputs=insrec, outputs=xcat)\n",
    "    cat_model.compile(loss=rec_cat, optimizer=Adam())\n",
    "    \n",
    "    return poisson_model, cat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_rec, cat_rec = regenerate_rec()\n",
    "\n",
    "def rec_poiss(y, y_):\n",
    "    return keras.losses.poisson(y[-1], y_[-1])\n",
    "\n",
    "def rec_cat(y, y_):\n",
    "    return keras.losses.categorical_crossentropy(y[-1], y_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_experiment(model, lossmthd, trainbatches, devbatches, maxtime):\n",
    "    \n",
    "    ei_patients = enumerate_cycle(trainbatches)\n",
    "    loss_history_rec = []\n",
    "    dev_loss_history_rec = []\n",
    "\n",
    "    # Accumulate some stats as we go along\n",
    "    #loss_history_l = []\n",
    "    #dev_loss_history_l = []\n",
    "    #dev_loss_history_l_fs = []\n",
    "\n",
    "    model.compile(loss=lossmthd, optimizer=Adam())\n",
    "    print('compiled model')\n",
    "    while not interrupted():\n",
    "        if(len(loss_history_rec) > 0 and loss_history_rec[-1][0]-loss_history_rec[0][0] >= maxtime):\n",
    "            break\n",
    "        (epoch,item),patient = next(ei_patients)\n",
    "        #print('got patient')\n",
    "        for _ in range(2):\n",
    "            loss = model.train_on_batch(patient[0][np.newaxis,:,:], patient[1][np.newaxis, :, np.newaxis])\n",
    "        #print('got loss')\n",
    "        #print(model.predict_on_batch(X[i]))\n",
    "        #input()\n",
    "        #loss = model.train_on_batch(T[i], trainy[i])\n",
    "        if np.isnan(loss): break\n",
    "        if item % 100 == 0:\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            print(f'loss={loss} epoch={epoch} batchnum={item}' + (f' time={loss_history_rec[-1][0]-loss_history_rec[0][0]}' if len(loss_history_rec) > 0 else ''))\n",
    "            loss_history_rec.append((time.time(), epoch, loss))\n",
    "            devp = devbatches[np.random.choice(np.arange(len(devbatches)))]\n",
    "           # print(ydprob[sample].shape,  model.predict_on_batch(D[sample]).numpy().shape)\n",
    "            devloss = tf.reduce_mean(lossmthd(devp[1], model.predict_on_batch(devp[0][np.newaxis, :, :])))\n",
    "            dev_loss_history_rec.append((time.time(), epoch+batch/len(indices), devloss))\n",
    "            if len(dev_loss_history_rec) > 20 and dev_loss_history_rec[-1][-1] > dev_loss_history_rec[-2][-1] and dev_loss_history_rec[-2][-1] > dev_loss_history_rec[-3][-1] and ((dev_loss_history_rec[-1][-1] > dev_loss_history_rec[-3][-1] + 0.075) or (dev_loss_history_rec[-1][-1] > min(map(lambda x: x[-1], dev_loss_history_rec)) + 0.1)):\n",
    "                print(\"FAAIIIIL\")\n",
    "                dev_loss_history_l_fs.append((time.time(), epoch, devloss))\n",
    "            else:\n",
    "                print()\n",
    "                                  \n",
    "    return loss_history_rec, dev_loss_history_rec, patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poisson_rec, _ = regenerate_rec()\n",
    "#ls, dls, patient = rec_experiment(poisson_rec, rec_poiss, train_batches_poisson, dev_batches_poisson, 600)\n",
    "#draw_losses(ls, dls, [], 'figures/rec_poiss_loss.pdf')\n",
    "_, cat_rec = regenerate_rec()\n",
    "cls, dcls, patient2 = rec_experiment(cat_rec, rec_cat, train_batches_cat, dev_batches_cat, 600)\n",
    "draw_losses(cls, dcls, [], 'figures/rec_cat_loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_losses(ls, dls, [], 'figures/rec_poisson_loss.pdf', comp=25)\n",
    "draw_losses(cls, dcls, [], 'figures/rec_cat_loss.pdf', comp=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_cat[345][0][np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_poiss = []\n",
    "\n",
    "for b in tqdm(val_batches_poisson[:, 0]):\n",
    "    \n",
    "    y_ = poisson_rec.predict_on_batch(b[np.newaxis, :, :])\n",
    "    ys_poiss.append(y_.numpy().reshape(-1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recvalev = np.array([list(map(lambda x: x[-1], val_batches_poisson[:, 1])), ys_])\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(recvalev[0], recvalev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cat_resps(model, valx, valy, file_name, yps_=None):\n",
    "    if(type(yps_) == 'NoneType'):\n",
    "        yps_ = model.predict_on_batch(valx).numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    print(yval == 9)\n",
    "    for _y in np.arange(1,15):\n",
    "        inds = valy == _y\n",
    "        print(yps_)\n",
    "        ys_rv = yps_[inds]\n",
    "       # print(ys_rv.shape)\n",
    "        responsibilities = ys_rv.mean(axis=0)\n",
    "       # print(responsibilities)\n",
    "        for i in np.arange(1,15):\n",
    "            ax.fill([_y-0.4875, _y-0.4875, _y+0.4875, _y+0.4875], [i-0.4875, i+0.4875, i+0.4875, i-0.4875], c='black', alpha=responsibilities[i-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_poiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_ = []\n",
    "for b in tqdm(val_batches_cat[:, 0]):\n",
    "    y_ = cat_rec.predict_on_batch(b[np.newaxis, :, :])\n",
    "    ys_.append(y_.numpy()[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pca_50_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recyval = np.array(list(map(lambda x: x[-1], )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recyval = []\n",
    "for l in val_batches_poisson[:, 1]:\n",
    "    recyval.extend(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_comparison(pca_85_cat_loss, pca_85_cat_loss, cls, '', comp=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-reduced_data.patient_nbr.value_counts().value_counts()[1]/len(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mlp_lrg_cat = large_model_cat.predict_on_batch(V).numpy().argmax(axis=1) + 1\n",
    "val_mlp_med_cat = med_model_cat.predict_on_batch(V).numpy().argmax(axis=1) + 1\n",
    "val_mlp_sml_cat = small_model_cat.predict_on_batch(V).numpy().argmax(axis=1) + 1\n",
    "\n",
    "val_mlp_lrg_poisson = large_model_poisson.predict_on_batch(V).numpy().reshape(-1)\n",
    "val_mlp_med_poisson = med_model_poisson.predict_on_batch(V).numpy().reshape(-1)\n",
    "val_mlp_sml_poisson = small_model_poisson.predict_on_batch(V).numpy().reshape(-1)\n",
    "\n",
    "val_pca_50_cat =  pca_cat_50.predict_on_batch(V_50).numpy().argmax(axis=1) + 1\n",
    "val_pca_85_cat =  pca_cat_85.predict_on_batch(V_85).numpy().argmax(axis=1) + 1\n",
    "val_pca_125_cat = pca_cat_125.predict_on_batch(V_125).numpy().argmax(axis=1) + 1\n",
    "\n",
    "val_pca_50_poisson = pca_poiss_50.predict_on_batch(V_50).numpy().reshape(-1)\n",
    "val_pca_85_poisson = pca_poiss_85.predict_on_batch(V_85).numpy().reshape(-1)\n",
    "val_pca_125_poisson = pca_poiss_125.predict_on_batch(V_125).numpy().reshape(-1)\n",
    "\n",
    "val_rec_cat = []\n",
    "val_rec_cat_prob = []\n",
    "yval_p = []\n",
    "for i, (b,y) in enumerate(tqdm(val_batches_cat)):\n",
    "    y_ = cat_rec.predict_on_batch(b[np.newaxis, :, :]).numpy().argmax(axis=2).reshape(-1)\n",
    "    val_rec_cat.append(y_[-1])\n",
    "    yval_p.append(y.reshape(-1)[-1])\n",
    "    val_rec_cat_prob.append(cat_rec.predict_on_batch(b[np.newaxis, :, :]).numpy()[-1])\n",
    "    \n",
    "print(y)\n",
    "val_rec_poiss = []\n",
    "yval_c = []\n",
    "for i, (b,y) in tqdm(enumerate(val_batches_poisson)):\n",
    "    y_ = poisson_rec.predict_on_batch(b[np.newaxis, :, :]).numpy().reshape(-1)\n",
    "    val_rec_poiss.append(y_[-1])\n",
    "    yval_c.append(y.reshape(-1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLP:')\n",
    "print('', 'Cat')\n",
    "print('', '', 'Large', pearsonr(val_mlp_lrg_cat, yval))\n",
    "print('', '', 'Mediu', pearsonr(val_mlp_med_cat, yval))\n",
    "print('', '', 'Small', pearsonr(val_mlp_sml_cat, yval))\n",
    "print('', 'Poiss')\n",
    "print('', '', 'Large', pearsonr(val_mlp_lrg_poisson, yval))\n",
    "print('', '', 'Mediu', pearsonr(val_mlp_med_poisson, yval))\n",
    "print('', '', 'Small', pearsonr(val_mlp_sml_poisson, yval))\n",
    "print('PCA:')\n",
    "print('', 'Cat')\n",
    "print('', '', '050', pearsonr(val_pca_50_cat, yval))\n",
    "print('', '', '085', pearsonr(val_pca_85_cat, yval))\n",
    "print('', '', '125', pearsonr(val_pca_125_cat, yval))\n",
    "print('', 'Poiss')\n",
    "print('', '', '050', pearsonr(val_pca_50_poisson, yval))\n",
    "print('', '', '085', pearsonr(val_pca_85_poisson, yval))\n",
    "print('', '', '125', pearsonr(val_pca_125_poisson, yval))\n",
    "print('Rec:')\n",
    "print('', 'X-Ent', pearsonr(val_rec_cat, yval_c))\n",
    "print('', 'Poiss', pearsonr(val_rec_poiss, yval_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yps_ = small_model_poisson.predict_on_batch(V).numpy()\n",
    "#ys_ = (yps_.argmax(axis=1) + 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for y, y_ in tqdm(zip(yval, yps_)):\n",
    "    ax.plot([y-0.5, y+0.5], [y_, y_], linewidth=0.5, c='black', alpha=0.1)   \n",
    "ax.set_xlim(0.5, 14.5)\n",
    "ax.set_ylim(0.5, 14.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('white')\n",
    "ax.spines['bottom'].set_color('white')\n",
    "list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "ax.set_xlabel('True Duration')\n",
    "ax.set_ylabel('Predicted Duration')\n",
    "ax.set_aspect('equal')\n",
    "print('saving now')\n",
    "plt.savefig('figures/small_dev_dis.pdf')\n",
    "print('saved')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yps_ = pca_cat_125.predict_on_batch(V_125).numpy()\n",
    "#ys_ = (yps_.argmax(axis=1) + 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for _y in np.arange(1,15):\n",
    "    inds = yval == _y\n",
    "    ys_rv = yps_[inds]\n",
    "    responsibilities = ys_rv.mean(axis=0)\n",
    "    for i in np.arange(1,15):\n",
    "        ax.fill([_y-0.4875, _y-0.4875, _y+0.4875, _y+0.4875], [i-0.4875, i+0.4875, i+0.4875, i-0.4875], c='black', alpha=responsibilities[i-1])\n",
    "        \n",
    "ax.set_xlim(0.5, 14.5)\n",
    "ax.set_ylim(0.5, 14.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('white')\n",
    "ax.spines['bottom'].set_color('white')\n",
    "list(map(lambda ts: ts.set_color('white'),ax.xaxis.get_majorticklines()))\n",
    "list(map(lambda ts: ts.set_color('white'),ax.yaxis.get_majorticklines()))\n",
    "ax.set_xlabel('True Duration')\n",
    "ax.set_ylabel('Predicted Duration')\n",
    "ax.set_aspect('equal')\n",
    "plt.savefig('figures/pca125_val_resps.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_model_cat.predict_on_batch(V).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rec.predict_on_batch(train_batches_cat[1352, 0][np.newaxis, :, :]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_rec.predict_on_batch(patient[0][np.newaxis, :, :]).numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.losses.poisson(np.array([4.]), np.array([0.04232]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yval_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(5)):\n",
    "    pass #poisson_rec.train_on_batch(train_batches_poisson[129, 0][np.newaxis, :, :], train)\n",
    "    \n",
    "print(poisson_rec.predict_on_batch(train_batches_poisson[4152,0][np.newaxis, :, :]).numpy(), train_batches_poisson[4152, 1])\n",
    "print(poisson_rec.predict_on_batch(train_batches_poisson[1146,0][np.newaxis, :, :]).numpy(), train_batches_poisson[1146, 1])\n",
    "print(poisson_rec.predict_on_batch(train_batches_poisson[3123,0][np.newaxis, :, :]).numpy())\n",
    "print(poisson_rec.predict_on_batch(patient[0][np.newaxis, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient[1][:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
