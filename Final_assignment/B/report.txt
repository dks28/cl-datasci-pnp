   L[]> ptm# C[]> ptm# R[]> ptm#           citsbib       capbtabboxtable[][]  Daniel Sääw dks Data Science: Principles and Practice,  Final Assignment B Lent    	 	 	 	  	Data Preparation  	The full data set from which a portion served as the foundation for Assignment A originally 	spanned  samples over  features   	Due to the nature of the task at hand, predicting a patient's next medical encounter's  	duration, of those samples I removed the patients that only occurred once, which  	eliminated about  of samples  of patients  	Once these preliminary selections had taken place, some initial investigations  	revealed that the shares of missing values differed significantly from Assignment A: I now  	retained the payercode feature,  	which had been missing in all but  samples in the data set for Assignment A only missed 	around  of values in this version of the data   	 	Overall, I made few other changes at this stage compared to my proceedings at Assignment A 	and any feature I do not mention explicitly here was treated in the same way 	The most notable changes I made to my preprocessing was to trust the neural networks to  	decide themselves which features are meaningful and which are not, so I included many other  	features in their raw, given form instead of modifying them as I had done previously This 	included the  features indicating the prescriptions of individual medications  	The results of this progress were as follows: a total of  samples remained    , whilst  	the total number of features changed to , of which  were numerical Overall, the  	numerical representation has  features  	Loss Function  	In this section, I describe two probability models I considered in my work These two models a Poisson model  	and a Categorical model, respectively I originally explored modelling the data as being  	binomially distributed, but observed poor performance 	 	I will speak of the label of the th sample  ie the  	duration of the next medical encounter with that patient as , where the associated  	random variable  follows some probability distribution  , and of the output of my machine-learning algorithms as 	  	The Poisson Model 	Formally, here, I mean to model my data as  	To fit such a model, we maximise the likelihood of its parameters as follows: 	* 		lik yi, xi  i PrYiyi  fxi   		 i e^-fxi fxi^yiyi!   		  likyi, xi 		 i-fxi + yifxi 	 	Empirically, the labels at hand seem to distribute roughly according to a Poisson  	distribution up to value , which suggests this very simple model might be suited for  	our purposes However, it should be noted that the Poisson distribution fails to capture  	the finite range of values the labels take and has a generally low variance 	The second model I will consider throughout this report may avoid these issues  	The Categorical Model 	The categorical distribution is the most general discrete probability distribution: there  	is no parameterised form, only an explicit probability vector, in this model specified by  	 If we now represent the label  	as the probability distribution , then we get that we can  	maximise the likelihood of  by maximising  	    	ie minimising the sum of the cross-entropies of the empirical probability distributions  	 and the predicted probability distributions   	This model makes full use of the generality of neural networks, by making the fewest  	possible assumptions about the data This, in combination with the added benefit of  	gaining an idea of a probability distribution rather than just obtaining a prediction  	might deliver improved performance at the cost of longer training times  	Machine-learning Algorithms Implementation         	         	In this section I describe the stages my network development underwent and the reasons I  	had for making different decisions about network architecture at various stages  	It should be noted that before I began any training procedures, I selected a two subsets of  	the data each containing roughly  of the data points based on their separation from  	the other data in a dimensionality-reduced representation of the data; see Fig 	   	Basic Deep Learning               	The first networks I constructed were very basic in their structure, comprising merely  	a number of densely connected layers before producing the output for the two respective  	probability models I trained three such networks: One with  internal  	layers each including between  and  neurons; one with  internal layers of  to  	neurons and one of  layers comprising  to  cells   	                   	                         	Measurement  	The first measurements are presented in Figs  and  	   	They outline both how the loss between the particular models' predictions and the actual  	labels develop, highlighting the overfitting that takes place, as well as how the losses  	develop for each underlying probability model in a comparison between the different-size  	networks  	The key take-aways are that while models trained using the Poisson loss function seem to  	overfit less strongly, those networks do not exhibit a real difference in performance 	with respect to loss between the different network sizes Meanwhile, such a difference  	can be seen in the networks when trained using cross-entropy, while the models all wildly 	overfit to the training data In general, we would also like the models to train faster  	than these do   	The question of which of the two loss functions provided the overall better results may be  	considered using Fig , which indicates the distributions of  	predictions with respect to true labels Note that since the network outputs a probability distribution when trained  	using cross-entropy, the corresponding subfigure indicates for each true label, the average 	predicted probability distribution In this case, there is no clear answer, since both  	underlying models seem to produce a tendency to majoritatively predict values of at most   	when applied to the development set after training, suggesting these models may not generalise well 	                    	Principal-Components Prediction and Regularization           	To try and improve the speed with which the models train, we reduce the complexity of the  	input space using PCA, considering three variations restricting the input space to the top  	,  or  principal components, respectively For the sake of brevity, we test this  	with the `large' model from before, a training acceleration of which would be the most  	desirable  	To address the more fervent issue of overfitting to the training data, we make use of  	a technique discussed in Machine Learning and Bayesian Inference, - 	regularization This imposes penalties on larger weights in the networks and so reduces  	the degree to which models can overfit   	Measurement 	 	The results of this are recorded in Figs ,   	We can immediately tell that the application of regularization helped with the problem of  	overfitting for the models trained with cross-entropy, as now the loss on the development  	set diverges much slower However, the models using Poisson loss were not so affected  	We can also see that these new models now converge to their eventual  	performance on the training set more quickly than before, indicating that the application  	of PCA succeeded in its mission      	                   	                                                                                	Recurrent Network Structures                           	The previous attempt modified the models slightly, whilst retaining the overall structure  	of the networks However, the available data set  is strictly richer than what the  	previous models used Because of the independence assumption for the samples in our data set,  	we have so far been neglecting all but the last encounters with a patient However,  	considering any particular patient individually, we actually have a sequence of encounters  	at our disposal when trying to predict the next encounter's duration  	We may try and exploit this fact using recurrent neural networks, 	so I considered possibilities to modify the model architecture to incorporate these I chose the  	foundation of this network as the large network from before after the application of PCA  	to  components and regularization  	Measurement 	The development of loss over time during training is presented in Fig , 	which contains two important pieces of information Firstly, the training process of the  	recurrent networks was slow     , despite the use of dimensionality reduction Secondly,  	the performance obtained fluctuated strongly Upon reflection, this is likely due to the  	fact that only about  of the patients of those who were encountered at least twice  	were encountered more than once This may have meant that perhaps the recurrent structure  for the networks was not as suitable as I had hoped   	                	However,  the recurrent models obtain comparable  	losses on the training set as the previous models incorporating PCA and regularization,  	while the development loss remains entirely on par with the training loss in the recurrent  	structure Intuitively this makes sense as taking a 	patient's entire medical history into account stabilises the input data to any model as  	compared to the previous setting, in the sense that, although the last encounters in the  	test set were chosen to be qualitatively different from the rest of the data, the  	previous visits may not have been, so by adopting this view of the world, the models  	become more resilient to circumstances being different between training data and later  	inputs from the real world  	Evaluation 	So, finally, I evaluate the efficacy of my ideas on the test set kept separate so far  	Figs ,  and  show the densities of  	predictions on the test set with respect to the true labels Numerically, we can determine a measure of correlation between  	the models' predictions and the test labels using Pearson's  This measure is  	recorded for each of the stages my investigation went through in Table    	Overall, we can see that none of the models seem to generalise particularly well and usually end up concentrating around predicting - days' stays  	 	Of course, this might be because my models were not suitable However, perhaps it is the case that a patient's 	previous hospital visits are not in general indicative of future encounters, since any number 	of days, months or perhaps years could pass between these In the face of these results,  	it should be remembered, however, that any real-world  	scenario involving questions like this would at least make the circumstances at the time  	of hospitalisation available to any machine learning application, which should have a lot  	of bearing on the eventual duration of the encounter  	                   	                  	        	                              