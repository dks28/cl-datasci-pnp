Data Exploration
Initial Decisions About Data Relevance And Missing Values
To begin, consider the task at hand. Since we ultimately aim to construct a mechanism for predicting
the readmission status of an arbitrary patient, the features concerning identification (encounter_id and
patient_nbr) are essentially noise to our task, so I removed these features.
Among the remaining features, several missed values. Most notably, perhaps, weight only recorded
categorical values for 300 samples. The feature payer_code was even more sparse, detailing a value for only
235 samples.
I thus judged both these features too sparse for meaningful analysis and decided to drop both of these features
as well.
The last feature obviously missing a significant portion of its values (36.4%) was medical_specialty. [1]
included this feature, filling missing values with a new class. Instead, for this and a number of other features
(e.g. admission_type_id) that encoded underlying missing values with some numerical ID (their share of
effectively missing vaues ranged between 11% and 22%), I decided to give no indication of value should it be
missing.
Finally, race was missing 180 values. Since there is no meaningful ‘average’ race with which these missing
values could have been imputed, I dropped those rows from the data set.
A further subset of the rows I dropped was that in which the discharge disposition makes the question
of readmission status (the question of interest) redundant; the categories corresponding to patient death or
transfer into a hospice. Thus, 9693 of the samples were retained, with the readmission classes now distributed
as ‘No’-32.2%, ‘> 30 days’-33.94% and ‘≤ 30 days’-33.86%.

Distributions of Categorical Features and Impact on Data Representation
The data set comprises mostly categorical features following different distributions, some of which are illustrated in figure 1. Note that for discharge disposition, admission type and admission source classes have been
grouped according to their meaning. This served to reduce the number of distinct classes for each of these
features, as well as improve the classes’ balance. Note also how both the A1C test and the glucose serum
tests were conducted rarely overall.
Especially features with many categories follow extremely unbalanced distributions, for example the
diagnoses. Figure 2 shows the distribution of diagnosis frequency, illustrating the multitude of diagnoses
as well as their frequencies’ highly heterogeneous distribution. This combination of attributes inhibits the
effectiveness of usual numerical representations of categorical data, and since the data set includes less than
half of possible diagnoses in ICD9, I decided to change their representation.
I thus consulted the ICD9 hierarchy of diagnoses to reduce the number of distinct classes to 20. Furthermore,
since the three features do not fundamentally differ semantically, I combined them into one feature listing
the classes of diagnoses made for each encounter.
Another set of features warranting further thought was that of ‘generic’ medications, comprising 22 features, each of which drew from 4 categories: ‘Up’, ‘Down’ and ‘Steady’, indicating that the dosage of the
medication was increased, decreased or kept steady, respectively, and ‘No’, indicating that the medication
was not prescribed. However, for the vast majority of samples, most medications were unprescribed, with
the generics having value ‘No’ for around 95.1% of samples on average, with Insulin being the most common
prescription (prescribed for around 48.8% of encounters) while Metformin was the second-most common,
prescribed only 15.9% of times.
Thus, my first idea for reducing this subset of features was to proceed similarly as before, clustering medications based on their similarities e.g. in terms of effect. However, my research showed that all were antidiabetic,
particularly meant for diabetes type II. Hence, I created features recording the number of medications the
dosages of which increased, decreased or remained steady, respectively, but not those not prescribed since
this would carry little additional information.
A final consideration: for the categorical features presented in figure 1, we should consider the distributions
of the target variable within each class, remembering the categories’ distributions. This information is
presented in figure 3. Significantly represented categories do provide interesting insights here: For example,
the data seems to carry little ethnic or gender bias, with reasonably high sample sizes inducing low deviation
from the overall distribution within the categories.
Secondly, age seems to correlate positively with readmission risk.
Finally: it seems that when the glucose serum test was at all conducted, the risk of readmission generally
increased. This, perhaps, indicates that scenarios where the medical professional felt the blood sugar test
was necessary justified more concern, premeditating readmission more strongly. Since conducting the test
was also rare, I joined the corresponding categories into one to balance the distribution of categories.

Preview of Classification Process
To get an insight into how easy the classification task might be for actual classifiers I plotted representations
of the data to potentially identify meaningful clustering depending on the readmission status. Using my
chosen data representation, which initially comprised 66 dimensions, I hence applied both PCA and tSNE
to obtain two-dimensional representations of the data. The results are laid out in figure 4. As can be seen,
this representation of the data does not seem to exhibit a clear clustering of encounters with particular
readmission statuses, indicating that my machine-learning implementations should perhaps be expected to
be weak learners.
It should be noted that the plot displaying the results of tSNE distorts the units in the two dimesnions
relative to each other. However, the tick marks on both axes are spaced equidistantly to give some idea of
the distance relationship.
Implementation and Application of Machine Learning Algorithms
Motivations and Initial Decisions
At this point, we should decide the exact problem to tackle and how to measure success.
Strack et al. conducted binary classification to identify cases of readmission within 30 days of the encounter.
The authors were “primarily interested in factors that lead to early readmission” [1]. However, machinelearning implementations might more conceivably be used with the purpose of guiding either discharge decisions or preparations for a potential readmission, not necessarily within 30 days. Initially, therefore, I pursued
the task of predicting whether a patient would be readmitted at all before moving on to multicategorical
classification.
This in mind, I decided to prioritise recall over precision for the binary classification task, since using a
machine-learning model to guide preparations for patient readmission or discharge decisions, the graver error
class would be false negatives, not false positives.

First Steps
First, I examined a number of standard classifiers to get an idea of the basic performance that was to
be expected. Their respective performances under 10-fold cross-validation over a training set are recorded
in table 1. Note especially that here, the majority classifier will always predict that the patient would
eventually be readmitted, leading to the perfect recall score. However, it should be noted that this is an

artefact of sampling the data set such that the three readmission statuses are equifrequent; the original data
set contained majoritatively encounters without record of readmission, such that a corresponding majority
classifier would have recall 0 for this task.
In light of these performances, I also considered the confusion matrices for the classifiers with very ‘good’
recall scores. These results are recorded in table 2, and highlight that the performances largely resulted from
the classifiers degenerating to near-majority-classifiers; obviously an undesirable outcome.
It should also be pointed out that the data in table 1 suggests that this is a reasonably hard task, as
none of the classifiers’ accuracies exceed that of the majority classifier by a large amount; with a number
of the classifiers underperforming. Following this evaluation, I dropped the naïve-Bayes classifier and the
perceptron from further consideration due to bad performance and redundance with the gradient-descent
classifier, respectively. However, I experimented with the decision tree, optimising the parameters of the
decision tree using a grid search.
This process found that, when optimising for recall, this classifier, too, would degenerate to the majority
classifier, is no particularly useful insight. Instead, I optimised for ‘balanced accuracy’, which measures
the recall values for each class and is meant for use with unbalanced data sets such as this one. Now, the
optimised decision tree has recall 82.14%, precision 73.4%, F1 -score 77.52% and accuracy 67.71%; still lower
than that of the majority classifier.
Subsequently, I investigated how using ensemble methods for this data set would perform. I retained the
logistic regression and gradient-descent classifiers from before as well as the optimised decision tree and the
k-neighbours classifier (which used k = 50) for voting classifiers using both hard and soft voting. Due to the
similarity between the majority classifier and the SVM, I also discarded the latter.

Ensemble Methods
My initial experimentation with ensemble methods was to replicate the decision tree in a random forest.
Indeed, as table 3 shows, the random forest did improve over the previous decision-tree performance.
More generally, I considered combining my chosen mechanisms’ predictions via voting. Thus, I evaluated
the performances of both a soft voter comprising the optimised decision tree, the logistic regression and the
k-neighbours-classifier and a hard voter comprising the former two and a gradient-descent classifier; with
performances outlined in table 3.
Notably, neither improve over the logistic regression in terms of accuracy. I therefore inspected how the
predictions of each of the constituent voters, the voting classifier and the true labels correlated. The most
common combinations of predictions and labels are listed in table 4. There, we can see (as expected) that
the most common cause for error was all voters yielding false positives. However, some of the most common
combinations of predictions and underlying label are actually most classifiers yielding a false negative. This
is peculiar, but indicates that there may be some subset of unreadmitted patients quite separate from others.
To investigate remedies, I also evaluated the performance of adaptive-boosting classifiers based on the hard5
voting classifier and the logistic regression and implemented a stacking classifier.
After some experimentation, I found that the most performant configuration for this was to combine the
predictions of the logistic regression, the decision tree and the k-nearest-neighbour classifiers using another
logistic regressor. The results of both the adaptive-boosting and the stacking classifiers are recorded in table
5.
Table 5 shows that this data set does not seem to be particularly conductive to adaptive boosting, as
both resulting classifiers seem to perform worse than their underlying original classifiers.
Table 5 also exhibits a phenomenon I have so far brushed over due to my original decision to prioritise
recall: the precision-recall trade-off.

The Precision-Recall Trade-Off and Further Evaluation
Let me highlight the following three classifiers in particular: The logistic regression, random forest discussed
briefly as well as my stacking classifier, each of which had recall exceeding 90% and among the highest accuracies on the data set so far, while both of these measures exceed the classifiers’ precisions score significantly.
In the real-world, we may care about investigating a more careful trade-off by tweaking how conservative the
classifiers are.
Investigating this leads to the results of figure 5, which show the development of precision and recall for
each of these classifiers with different degrees of conservativeness, as well as the development of precision
with increasing recall. The trade-off nature between the two measures is clearly visible in these graphs, with
a general tendency to see a more rapid decline in recall than the corresponding rise in precision due the
unbalanced nature of the data set.
Since these classifiers behave almost identically for this data set in terms of the precision-recall trade-off,
I also considered another means of evaluation for the performance on this data set, the receiver-operating
characteristic (ROC), which by the area-under-curve (AUC) measurement, offers a numerical judgement
about classifier quality in the face of qualitatively similar performance as is the case here. The ROC curves
for each of these three classifiers, as well as their respective AUC scores are given in figure 6. This figure
reaffirms that the classifiers’ performances are very similar; the AUC measure however credits the randomforest classifier with a narrow advantage over the other two, that had previously slightly outperformed it
without considering these trade-offs.

Multicategorical Classification
Of course, the more refined problem of predicting a time frame for readmission is also of interest. I again
retained the three classifiers from the previous section due to their relatively high accuracies, as well has the
hard-voting classifier that also exceeded 70% accuracy in binary classification.
These classifiers’ accuracies are recorded in table 6a. Although accuracies of around 50% may seem
discouragingly low at first, I should point out that a majority classifier here would merely be accurate around
34% of the time, meaning that we do see a fairly considerable improvement over the most naïve classifier
here, especially considering the early indication that the data would not be easily separable.
I also considered the possibility of reducing the number of features in the data to potentially improve
performance; I therefore isolated the 20 features deemed most significant by the random-forest classifier
and evaluated whether using these might help improve accuracy. I reëxamined dimensionality reduction

to investigate whether the extraneous features had obscured some underlying clustering. The results are
displayed in figure 7, which shows that this was not the case to an extent obvious to the naked eye. I
proceeded to evaluate the classifiers from before on the reduced training data, with the results presented in
table 6b, which shows that reducing the data set in this way does not improve performance after all.
As a final piece of analysis, consider figure 8, which indicates, for each of the classifiers, the frequencies
of the different kinds of misclassification. These results indicate that misclassification of encounters where
readmission had been recorded was comparably likely to be as the other category of readmission or as no
predicted readmission, which is perhaps surprising as a layman’s intuition might suggest that readmission
within thirty days may be more similar to later readmission than no readmission at all, but for the hypothesis
that the categories are simply hard to separate.
Final Evaluation
To conclude, I present a brief evaluation of these classifiers on a test set that had been isolated from the
development process. This evaluation should yield some indication of how performant the algorithms might
be when applied to a real-world test with genuinely new data. The results of this are presented in table 7 and
suggest that the classifiers were not overfit to the training set and so are fairly robust to new data, since they
reflect a slight improvement over the results we had obtained for the training data. This implies that our
previous results were representative of a potential real-world application under this empirical distribution of
readmission records.
