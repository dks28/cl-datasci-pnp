{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 5: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will continue from where the lecture left off and learn more about using TensorFlow. \n",
    "\n",
    "The practical will cover a few different network architectures and we will look at different components that are often used in neural networks.\n",
    "\n",
    "To start off, let's import TensorFlow into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dks28/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TensorFlow v2 was only released recently.  The v1 API is still available as a submodule, although we won't be using that in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v2.compat.v1' from '/home/dks28/.local/lib/python3.6/site-packages/tensorflow/_api/v2/compat/v1/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal TensorFlow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first example from the lecture. We first create a network takes an input vector, multiplies it by a weight matrix, adds a weight vector, and returns the result.\n",
    "\n",
    "`tf.Variable` defines model parameters, which can be trained (as we will see shortly).  Here, we initialise the matrix variable as a 3x3 matrix, with every entry as 1.  Meanwhile, we initialise the vector variable with every entry as 0.\n",
    "`tf.linalg.matvec` multiplies a matrix and a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([12. 12. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = tf.Variable(tf.ones(shape=(3,3)))\n",
    "weight_vector = tf.Variable(tf.zeros(shape=(3,)))\n",
    "\n",
    "def affine_transformation(input_vector):\n",
    "    return tf.linalg.matvec(weight_matrix, input_vector) + weight_vector\n",
    "\n",
    "result = affine_transformation([2.,3.,7.])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second example from the lecture, showing how to optimise the parameters in your model.\n",
    "\n",
    "We first define a network that takes an input vector, multiplies it with a matrix (defined above), and sums the elements of the resulting vector (using `tf.math.reduce_sum`).  We then define a loss function, as the square error.  Given a specific input and output, we can calculate the loss of applying the network to the input.\n",
    "\n",
    "Next, we define an optimiser &ndash; here, we are using stochastic gradient descent (SGD) with learning rate 0.001.  We then use this optimiser to train this network for 10 epochs, over this single training point.  This optimises the output towards the target value 20.  Printing out the results, we can see that the output gradually moves towards the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(29.952, shape=(), dtype=float32)\n",
      "tf.Tensor(26.190144, shape=(), dtype=float32)\n",
      "tf.Tensor(23.850271, shape=(), dtype=float32)\n",
      "tf.Tensor(22.39487, shape=(), dtype=float32)\n",
      "tf.Tensor(21.489607, shape=(), dtype=float32)\n",
      "tf.Tensor(20.926535, shape=(), dtype=float32)\n",
      "tf.Tensor(20.576305, shape=(), dtype=float32)\n",
      "tf.Tensor(20.358461, shape=(), dtype=float32)\n",
      "tf.Tensor(20.222961, shape=(), dtype=float32)\n",
      "tf.Tensor(20.138683, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def network(input_vector):\n",
    "    return tf.math.reduce_sum(affine_transformation(input_vector))\n",
    "\n",
    "def loss_fn(predicted, gold):\n",
    "    return tf.square(predicted - gold)\n",
    "\n",
    "input = [2.,3.,7.]\n",
    "gold_output = 20\n",
    "\n",
    "def loss():\n",
    "    return loss_fn(network(input), gold_output)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    opt.minimize(loss, var_list=[weight_matrix, weight_vector])\n",
    "    print(network(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most cases, we don't actually need to create the trainable variables manually. Instead, the feedfoward layer is available as a pre-defined module.\n",
    "\n",
    "We can define a network as a sequence of operations, using `tf.keras.Sequential`.  The first operation here is a dense feedforward layer (`tf.keras.layers.Dense`), which acts like the `affine_transfomation` function we defined earlier.  The second operation sums the elements of the vector &ndash; this isn't a standard operation, so we have used `tf.keras.layers.Lambda` to allow a user-defined function.\n",
    "\n",
    "By default, the parameters in a layer (like `tf.keras.layers.Dense`) are initialised randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_shape=(3,)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that such a model expects the input data to be given as a *minibatch* &ndash; this means that the input tensor should have an extra index, which ranges over datapoints.  In our case, instead of passing a 3-dimensional input vector, we have to pass an Nx3 matrix, where N is the number of datapoints.  Here, we can apply the model to a single datapoint (a 1x3 matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.488218], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tf.constant([[2.,3.,7.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model defined in terms of layers, let's replace the manually created variables of the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([9.799363], shape=(1,), dtype=float32)\n",
      "tf.Tensor([13.655205], shape=(1,), dtype=float32)\n",
      "tf.Tensor([16.053537], shape=(1,), dtype=float32)\n",
      "tf.Tensor([17.545301], shape=(1,), dtype=float32)\n",
      "tf.Tensor([18.473175], shape=(1,), dtype=float32)\n",
      "tf.Tensor([19.050314], shape=(1,), dtype=float32)\n",
      "tf.Tensor([19.409298], shape=(1,), dtype=float32)\n",
      "tf.Tensor([19.632582], shape=(1,), dtype=float32)\n",
      "tf.Tensor([19.771467], shape=(1,), dtype=float32)\n",
      "tf.Tensor([19.857853], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(predicted, gold):\n",
    "    return tf.square(predicted - gold)\n",
    "\n",
    "input = tf.constant([[2.,3.,7.]])\n",
    "gold_output = 20\n",
    "\n",
    "def loss():\n",
    "    return loss_fn(model(input), gold_output)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    opt.minimize(loss, var_list=model.trainable_variables)\n",
    "    print(model(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, for standard optimizers and loss functions, the TensorFlow API makes it even easier for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.528937, shape=(), dtype=float32)\n",
      "tf.Tensor(12.864999, shape=(), dtype=float32)\n",
      "tf.Tensor(15.562031, shape=(), dtype=float32)\n",
      "tf.Tensor(17.239582, shape=(), dtype=float32)\n",
      "tf.Tensor(18.28302, shape=(), dtype=float32)\n",
      "tf.Tensor(18.93204, shape=(), dtype=float32)\n",
      "tf.Tensor(19.335726, shape=(), dtype=float32)\n",
      "tf.Tensor(19.586824, shape=(), dtype=float32)\n",
      "tf.Tensor(19.743006, shape=(), dtype=float32)\n",
      "tf.Tensor(19.840149, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, input_shape=(3,)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "input = tf.constant([[2.,3.,7.]])\n",
    "gold_output = tf.constant([[20.]])\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train_on_batch(input, gold_output)\n",
    "    print(model(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the lecture, activation functions are what gives neural networks their power to model non-linear patterns in the data.  After applying an affine transformation, we then apply a non-linear activation function to each element.\n",
    "\n",
    "There are a number of different activation functions to choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**sigmoid** function](https://en.wikipedia.org/wiki/Logistic_function), also known as the logistic function, transforms the value into the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f6e306cca58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense(100, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**tanh** function](https://en.wikipedia.org/wiki/Hyperbolic_function) has a similar shape to the sigmoid function, but transforms the value into the range between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f6e306cc550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense(100, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\">**Rectified Linear Unit** function</a>, or ReLU, is the identity for positive values, but maps all negative values to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f6e3957e5f8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense(100, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks, an important activation function is the [**softmax**](https://en.wikipedia.org/wiki/Softmax_function).  This is unlike the activation functions mentioned above, because it isn't applied to each element separately.  It converts a vector of scores into a probability distribution &ndash; after applying the softmax, all values are between 0 and 1, and together they sum to 1.  Higher scores are assigned to higher probabilities, via the formula:\n",
    "\n",
    "$P(i) \\propto \\exp(x_i)$\n",
    "\n",
    "Or, more explicitly (notice how the value of the denominator depends on all other values):\n",
    "\n",
    "$P(i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
    "\n",
    "The softmax is often used in the output layer of a network performing classification, in order to predict a probability distribution over all the possible classes.  For example, the following model takes a 20-dimensional input, maps it to a 50-dimensional hidden layer, then maps that to a distribution over 10 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, input_shape=(20,), activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations and Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has corresponding versions of all the main operations you might want to use. This means you can add them into your computation graph and into your neural network.  The most common operations are available in `tf`, and further operations are available in `tf.math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.gen_math_ops.exp(x, name=None)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.abs                 # absolute value\n",
    "tf.negative            # computes the negative value\n",
    "tf.sign                # returns 1, 0 or -1 depending on the sign of the input\n",
    "tf.math.reciprocal     # reciprocal 1/x\n",
    "tf.square              # return input squared\n",
    "tf.round               # return rounded value\n",
    "tf.sqrt                # square root\n",
    "tf.math.rsqrt          # reciprocal of square root\n",
    "tf.pow                 # power\n",
    "tf.exp                 # exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations can be applied to scalar values, but also to vectors, matrices and higher-order tensors. In the latter case, they will be applied element-wise. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-3.2  2.7], shape=(2,), dtype=float32)\n",
      "tf.Tensor([2.25      4.4099994], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.negative([3.2,-2.7]))\n",
    "print(tf.square([1.5,-2.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful operations are performed over a whole vector/matrix tensor and return a single value (we saw `tf.reduce_sum` earlier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.math_ops.argmin_v2(input, axis=None, output_type=tf.int64, name=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum # Add elements together\n",
    "tf.reduce_mean # Average over elements\n",
    "tf.reduce_min # Minimum value\n",
    "tf.reduce_max # Maximum value\n",
    "tf.argmax # Index of the largest value\n",
    "tf.argmin # Index of the smallest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used stochastic gradient descent (SGD) to train our model.  This uses a fixed learning rate to update the parameters.  Several optimisation algorithms are based on SGD, but adaptively adjust the learning rate (usually for each parameter separately).\n",
    "\n",
    "Different adaptive learning rate strategies are also implemented in TensorFlow as functions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.optimizers.SGD\n",
    "tf.keras.optimizers.Adadelta\n",
    "tf.keras.optimizers.Adam\n",
    "tf.keras.optimizers.RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in the differences between these strategies, [this blog post](http://ruder.io/optimizing-gradient-descent/) provides more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an XOR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XOR](https://en.wikipedia.org/wiki/XOR_gate) is the function that takes two binary values and returns 1 if one of them is 1 and the other 0, while returning 0 if both of them have the same value.\n",
    "\n",
    "It can be a difficult function to learn and cannot be modelled with a linear model. But let's try anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of all the possible different states that XOR can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_input = tf.constant([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "xor_output = tf.constant([0.0, 1.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a linear network and optimize it on this dataset, printing out the predictions at each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 epochs: [-0.21557409  0.53843343  0.2679081   1.0219157 ]\n",
      "after 20 epochs: [-0.02778651  0.49849507  0.3365215   0.8628031 ]\n",
      "after 30 epochs: [0.11615891 0.4882609  0.39128137 0.7633833 ]\n",
      "after 40 epochs: [0.22096115 0.4852433  0.42717808 0.69146025]\n",
      "after 50 epochs: [0.29715112 0.48554987 0.4507841  0.63918287]\n"
     ]
    }
   ],
   "source": [
    "linear_model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(2,))])\n",
    "\n",
    "linear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "                     loss='mean_squared_error')\n",
    "\n",
    "for epoch in range(50):\n",
    "    linear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), linear_model(xor_input).numpy().reshape((4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not doing very well. Ideally, the predictions should be \\[0, 1, 1, 0\\], but in this case they are hovering around 0.5 for every input case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve this architecture, let's add some non-linear layers into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 epochs: [0.4512474 0.5330985 0.594556  0.5184007]\n",
      "after 20 epochs: [0.41759464 0.5759795  0.65515167 0.46746808]\n",
      "after 30 epochs: [0.37860784 0.65029824 0.71506745 0.39570573]\n",
      "after 40 epochs: [0.31847364 0.726701   0.773647   0.32191446]\n",
      "after 50 epochs: [0.2764116  0.8051618  0.8178675  0.27033544]\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='mean_squared_error')\n",
    "\n",
    "for epoch in range(50):\n",
    "    nonlinear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy().reshape((4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better. The values are much closer to \\[0, 1, 1, 0\\] than before, and they will continue improving if we train for longer.  (But remember that the model is initialised randomly &ndash; if you run it a few times, you will see that the results vary with each run.)\n",
    "\n",
    "Notice that we have used a higher learning rate for this network. It will still learn with a smaller learning rate, but will converge more slowly. As discussed in the lecture, the learning rate is a hyperparameter that can vary quite a bit depending on the network architecture and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do classification with TensorFlow. For this, we often use the softmax activation function described above, which predicts the probability for each of the possible classes.\n",
    "\n",
    "We also have to change the loss function, as squared error is not suitable for classification.  A suitable loss function is [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy).  Because the correct output has probability 1 for the correct class, and probability 0 for the rest, cross entropy is the same as the negative log probability of the correct class.  In other words, by minimising cross entropy, we are trying to find the maximum likelihood model.\n",
    "\n",
    "We can change the XOR example above to perform classification instead.  In this case, we are constructing a binary classifier &ndash; choosing between the classes of 0 and 1.  The output now prints the predicted probabilities of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dks28/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "after 10 epochs:\n",
      "[[0.5117795  0.4882205 ]\n",
      " [0.18200074 0.8179992 ]\n",
      " [0.5093838  0.49061623]\n",
      " [0.65182793 0.34817204]]\n",
      "after 20 epochs:\n",
      "[[0.5365528  0.46344724]\n",
      " [0.09488795 0.905112  ]\n",
      " [0.5270997  0.4729003 ]\n",
      " [0.82926255 0.17073746]]\n",
      "after 30 epochs:\n",
      "[[0.544068   0.45593202]\n",
      " [0.0363895  0.9636105 ]\n",
      " [0.4307922  0.56920785]\n",
      " [0.9330242  0.06697579]]\n",
      "after 40 epochs:\n",
      "[[0.68452746 0.31547254]\n",
      " [0.0463866  0.9536134 ]\n",
      " [0.1929348  0.8070652 ]\n",
      " [0.9329038  0.06709618]]\n",
      "after 50 epochs:\n",
      "[[0.81181437 0.18818559]\n",
      " [0.03061958 0.9693804 ]\n",
      " [0.08298508 0.91701496]\n",
      " [0.96215767 0.03784232]]\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "for epoch in range(50):\n",
    "    nonlinear_model.train_on_batch(xor_input, xor_output)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatching\n",
    "\n",
    "For the XOR data, there are only 4 datapoints.  However, with realistic datasets, it is inefficient to train on the whole dataset at once, because this will require a lot of computation in order to make a single update step.  Instead, we can train on a batch of data at a time.  For example, taking batches of 2 datapoints for the XOR data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 epochs:\n",
      "[[0.5654291  0.43457085]\n",
      " [0.5125258  0.48747417]\n",
      " [0.1469599  0.8530401 ]\n",
      " [0.7588526  0.24114738]]\n",
      "after 20 epochs:\n",
      "[[0.78188807 0.21811192]\n",
      " [0.0782566  0.9217434 ]\n",
      " [0.02357241 0.9764276 ]\n",
      " [0.9759774  0.02402258]]\n",
      "after 30 epochs:\n",
      "[[0.9252303  0.07476968]\n",
      " [0.02335044 0.9766495 ]\n",
      " [0.01326589 0.98673403]\n",
      " [0.9908187  0.00918128]]\n",
      "after 40 epochs:\n",
      "[[0.9599664  0.04003363]\n",
      " [0.01128379 0.9887162 ]\n",
      " [0.00891465 0.99108535]\n",
      " [0.99415094 0.00584907]]\n",
      "after 50 epochs:\n",
      "[[0.97475106 0.02524896]\n",
      " [0.00826528 0.99173474]\n",
      " [0.00618102 0.99381894]\n",
      " [0.9962547  0.00374533]]\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "for epoch in range(50):\n",
    "    for i in range(0,len(xor_input),BATCH_SIZE):\n",
    "        input_batch = xor_input[i:i+BATCH_SIZE]\n",
    "        output_batch = xor_output[i:i+BATCH_SIZE]\n",
    "        nonlinear_model.train_on_batch(input_batch, output_batch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('after {} epochs:'.format(epoch+1), nonlinear_model(xor_input).numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this kind of functionality is built into TensorFlow.  The following code trains the model with the given batch size and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "Epoch 1/40\n",
      "4/4 [==============================] - 0s 7ms/sample - loss: 0.9689\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 1.1063\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - 0s 953us/sample - loss: 0.7695\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - 0s 921us/sample - loss: 0.9616\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - 0s 957us/sample - loss: 1.0159\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.6964\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6233\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5942\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5969\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - 0s 968us/sample - loss: 0.5985\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5435\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5673\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5271\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5030\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - 0s 846us/sample - loss: 0.4875\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5331\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5634\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5982\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5658\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5430\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6239\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5279\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5134\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - 0s 870us/sample - loss: 0.5078\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5107\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5134\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5075\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6018\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5165\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6261\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5082\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5100\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6203\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5256\n",
      "Epoch 35/40\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5172\n",
      "Epoch 36/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5089\n",
      "Epoch 37/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5114\n",
      "Epoch 38/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6167\n",
      "Epoch 39/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.6170\n",
      "Epoch 40/40\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5330\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.4855\n",
      "final loss: 0.485472172498703\n",
      "final predictions:\n",
      "[[0.72563225 0.27436778]\n",
      " [0.00715317 0.9928469 ]\n",
      " [0.72563225 0.27436778]\n",
      " [0.72563225 0.27436778]]\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(2,), activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "nonlinear_model.fit(xor_input, xor_output, batch_size=2, epochs=40)\n",
    "\n",
    "print('final loss:', nonlinear_model.evaluate(xor_input, xor_output))\n",
    "print('final predictions:', nonlinear_model.predict(xor_input), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Classification of House Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first practical, you used the California House Prices Dataset in order to predict the prices of the houses based on various properties about the houses. In this assignment, we will experiment with TensorFlow and train a model to predict the \"ocean proximity\" of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>-122.12</td>\n",
       "      <td>37.69</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2227.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>2.3973</td>\n",
       "      <td>167300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>-118.45</td>\n",
       "      <td>34.18</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>3.6875</td>\n",
       "      <td>246400.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16293</th>\n",
       "      <td>-121.23</td>\n",
       "      <td>37.96</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>1.6563</td>\n",
       "      <td>57200.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>-118.27</td>\n",
       "      <td>33.99</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>1783.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>1.8086</td>\n",
       "      <td>97100.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>-119.73</td>\n",
       "      <td>36.68</td>\n",
       "      <td>32.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>681.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>1.7986</td>\n",
       "      <td>49300.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "699      -122.12     37.69                10.0       2227.0           560.0   \n",
       "3848     -118.45     34.18                34.0       1843.0           442.0   \n",
       "16293    -121.23     37.96                37.0       2351.0           564.0   \n",
       "4941     -118.27     33.99                38.0       1407.0           447.0   \n",
       "2053     -119.73     36.68                32.0        755.0           205.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "699        1140.0       472.0         2.3973            167300.0   \n",
       "3848        861.0       417.0         3.6875            246400.0   \n",
       "16293      1591.0       549.0         1.6563             57200.0   \n",
       "4941       1783.0       402.0         1.8086             97100.0   \n",
       "2053        681.0       207.0         1.7986             49300.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "699          NEAR BAY  \n",
       "3848        <1H OCEAN  \n",
       "16293          INLAND  \n",
       "4941        <1H OCEAN  \n",
       "2053           INLAND  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../DSPNP_practical1/housing/housing.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the ocean proximity column from the other features and convert the values to numerical IDs. Remember, the ocean_proximity column already contains discrete classes, so it is well-suited for the classification task. However, these are strings and in order to optimize the softmax function in TensorFlow, we need numerical IDs instead of strings. We can use the pandas map function to do the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.copy().drop([\"ocean_proximity\"], axis=1)\n",
    "Y = data.copy()[\"ocean_proximity\"].map({\"<1H OCEAN\":0, \"INLAND\":1, \"ISLAND\": 2, \"NEAR BAY\": 3, \"NEAR OCEAN\": 4}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split off some data for development and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.model_selection, sklearn.impute\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=28)\n",
    "X_train, X_dev, Y_train, Y_dev = sk.model_selection.train_test_split(X_train, Y_train, test_size=0.2, train_size=0.8, random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's preprocess the input features before giving them to the network. We need to fill in missing values with the imputer, and standardize the values to a similar range using the scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = sk.impute.SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train = imputer.transform(X_train)\n",
    "X_dev = imputer.transform(X_dev)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "scaler = sk.preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_dev = scaler.transform(X_dev)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset that we can work with. \n",
    "\n",
    "Input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13209, 9)\n",
      "(3303, 9)\n",
      "(4128, 9)\n",
      "[[ 1.15689593 -0.71445053  0.34720488 -0.64577547 -0.58476315 -0.49719495\n",
      "  -0.67595606 -1.38919759 -1.27249527]\n",
      " [-1.38215885  0.89977649  1.85989223 -0.25794142 -0.45025071 -0.54065637\n",
      "  -0.49431384  0.57927013  2.54593863]\n",
      " [ 0.31386989 -0.12194524  0.50643512 -0.18439456 -0.34877641 -0.44851816\n",
      "  -0.32564607 -0.14257305 -0.96144134]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the correstponding gold-standard labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13209,)\n",
      "(3303,)\n",
      "(4128,)\n",
      "[1 4 1 0 0 4 1 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(Y_dev.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code examples above, construct a TensorFlow model, then train, tune and test it on this dataset. Experiment with different model settings and hyperparameters. Calculate and evaluate classification accuracy - the percentage of datapoints where the predicted class matches the gold-standard class.\n",
    "\n",
    "During the practical session, give examples of what you tried and what were your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some suggestions and tips:\n",
    " * The XOR classification code can be a good place to start.\n",
    " * The output layer needs to have size 5, because the dataset has 5 possible classes.\n",
    " * Try testing on the development set as you are training, to make sure you don't overfit.\n",
    " * Evaluate on the dev set as much as you want, but evaluate on the test set only after you have chosen a good set of hyperparameters.\n",
    " * You could try different learning rates, hidden layer sizes, learning strategies, etc.\n",
    " * Adaptive learning rates can (and sometimes should) be used together with a regular hand-picked learning rate, and different adaptive learning rates can prefer very different regular learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13209 samples\n",
      "Epoch 1/40\n",
      "13209/13209 [==============================] - 0s 5us/sample - loss: 1.2007\n",
      "Epoch 2/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.9464\n",
      "Epoch 3/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8587\n",
      "Epoch 4/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8045\n",
      "Epoch 5/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7667\n",
      "Epoch 6/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7303\n",
      "Epoch 7/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7065\n",
      "Epoch 8/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6864\n",
      "Epoch 9/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6711\n",
      "Epoch 10/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6575\n",
      "Epoch 11/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6453\n",
      "Epoch 12/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6330\n",
      "Epoch 13/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6236\n",
      "Epoch 14/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6127\n",
      "Epoch 15/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6061\n",
      "Epoch 16/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5952\n",
      "Epoch 17/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5874\n",
      "Epoch 18/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5809\n",
      "Epoch 19/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5710\n",
      "Epoch 20/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5648\n",
      "Epoch 21/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5563\n",
      "Epoch 22/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5487\n",
      "Epoch 23/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5426\n",
      "Epoch 24/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5349\n",
      "Epoch 25/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5282\n",
      "Epoch 26/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5219\n",
      "Epoch 27/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5138\n",
      "Epoch 28/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5078\n",
      "Epoch 29/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5004\n",
      "Epoch 30/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4955\n",
      "Epoch 31/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.4885\n",
      "Epoch 32/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4839\n",
      "Epoch 33/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.4790\n",
      "Epoch 34/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4736\n",
      "Epoch 35/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4677\n",
      "Epoch 36/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4625\n",
      "Epoch 37/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4579\n",
      "Epoch 38/40\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.4527\n",
      "Epoch 39/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.4480\n",
      "Epoch 40/40\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.4439\n",
      "3303/3303 [==============================] - 0s 31us/sample - loss: 0.4288\n",
      "final loss: 0.4288221789992581\n",
      "final responsibilities:\n",
      "[[9.48291868e-02 9.00334179e-01 1.07743756e-04 3.62093351e-03\n",
      "  1.10791053e-03]\n",
      " [8.33442271e-01 4.67154384e-03 6.89506269e-05 7.42673175e-04\n",
      "  1.61074638e-01]\n",
      " [4.50458407e-04 9.99548256e-01 1.99382200e-07 1.66519385e-07\n",
      "  9.14955706e-07]\n",
      " [9.12196100e-01 4.94983979e-02 1.56282640e-05 6.84234110e-05\n",
      "  3.82214449e-02]\n",
      " [8.61013293e-01 3.09970547e-02 3.08135641e-05 9.74766590e-05\n",
      "  1.07861347e-01]]\n",
      "final predictions: tf.Tensor([1 0 1 0 0], shape=(5,), dtype=int64) [1 0 1 0 0]\n",
      "accuracy on dev set: 0.8392370572207084\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "nonlinear_model_2l = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, input_shape=(9,), activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model_2l.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "# nonlinear_model_2l.fit(X_train, Y_train, batch_size=3000, epochs=40) # acc=0.7914\n",
    "# nonlinear_model_2l.fit(X_train, Y_train, batch_size=2000, epochs=40) # acc=0.8062\n",
    "nonlinear_model_2l.fit(X_train, Y_train, batch_size=1000, epochs=40) # acc=0.8392\n",
    "\n",
    "print('final loss:', nonlinear_model_2l.evaluate(X_dev, Y_dev))\n",
    "print('final responsibilities:', nonlinear_model_2l.predict(X_dev[:5]), sep='\\n')\n",
    "print('final predictions:', tf.argmax(nonlinear_model_2l.predict(X_dev[:5]), axis=1), Y_dev[:5])\n",
    "print('accuracy on dev set:', (np.array(tf.argmax(nonlinear_model_2l.predict(X_dev), axis=1))==Y_dev).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13209 samples\n",
      "Epoch 1/80\n",
      "13209/13209 [==============================] - 0s 6us/sample - loss: 0.1783\n",
      "Epoch 2/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1768\n",
      "Epoch 3/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1748\n",
      "Epoch 4/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1768\n",
      "Epoch 5/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1789\n",
      "Epoch 6/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1762\n",
      "Epoch 7/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1771\n",
      "Epoch 8/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1759\n",
      "Epoch 9/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1751\n",
      "Epoch 10/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1760\n",
      "Epoch 11/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1750\n",
      "Epoch 12/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1766\n",
      "Epoch 13/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1746\n",
      "Epoch 14/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1776\n",
      "Epoch 15/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1787\n",
      "Epoch 16/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1735\n",
      "Epoch 17/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1754\n",
      "Epoch 18/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1753\n",
      "Epoch 19/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1748\n",
      "Epoch 20/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1743\n",
      "Epoch 21/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1723\n",
      "Epoch 22/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1753\n",
      "Epoch 23/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1779\n",
      "Epoch 24/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1739\n",
      "Epoch 25/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1742\n",
      "Epoch 26/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1751\n",
      "Epoch 27/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1752\n",
      "Epoch 28/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1738\n",
      "Epoch 29/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1752\n",
      "Epoch 30/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1751\n",
      "Epoch 31/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1772\n",
      "Epoch 32/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1753\n",
      "Epoch 33/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1752\n",
      "Epoch 34/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1765\n",
      "Epoch 35/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1724\n",
      "Epoch 36/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1798\n",
      "Epoch 37/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1761\n",
      "Epoch 38/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1740\n",
      "Epoch 39/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1717\n",
      "Epoch 40/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1763\n",
      "Epoch 41/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1755\n",
      "Epoch 42/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1732\n",
      "Epoch 43/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1759\n",
      "Epoch 44/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1771\n",
      "Epoch 45/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1764\n",
      "Epoch 46/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1729\n",
      "Epoch 47/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1715\n",
      "Epoch 48/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1711\n",
      "Epoch 49/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1749\n",
      "Epoch 50/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1731\n",
      "Epoch 51/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1719\n",
      "Epoch 52/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1748\n",
      "Epoch 53/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1710\n",
      "Epoch 54/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1728\n",
      "Epoch 55/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1711\n",
      "Epoch 56/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1750\n",
      "Epoch 57/80\n",
      "13209/13209 [==============================] - ETA: 0s - loss: 0.227 - 0s 2us/sample - loss: 0.1768\n",
      "Epoch 58/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1780\n",
      "Epoch 59/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1729\n",
      "Epoch 60/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1721\n",
      "Epoch 61/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1732\n",
      "Epoch 62/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1754\n",
      "Epoch 63/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1735\n",
      "Epoch 64/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1739\n",
      "Epoch 65/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1825\n",
      "Epoch 66/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1716\n",
      "Epoch 67/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1740\n",
      "Epoch 68/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1728\n",
      "Epoch 69/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1712\n",
      "Epoch 70/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1767\n",
      "Epoch 71/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1723\n",
      "Epoch 72/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1704\n",
      "Epoch 73/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1704\n",
      "Epoch 74/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1704\n",
      "Epoch 75/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1739\n",
      "Epoch 76/80\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.1727\n",
      "Epoch 77/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1717\n",
      "Epoch 78/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1728\n",
      "Epoch 79/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1723\n",
      "Epoch 80/80\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.1715\n",
      "3303/3303 [==============================] - 0s 30us/sample - loss: 0.1904\n",
      "final loss: 0.1904120722477125\n",
      "final responsibilities:\n",
      "[[1.01172463e-05 9.99989867e-01 3.93068599e-11 3.61849084e-09\n",
      "  3.71290540e-15]\n",
      " [9.91770029e-01 2.33378116e-04 6.81529811e-04 3.13505938e-04\n",
      "  7.00146379e-03]\n",
      " [4.94006969e-08 1.00000000e+00 8.64208965e-12 2.86306406e-13\n",
      "  6.00301885e-16]\n",
      " [9.68763769e-01 3.11843101e-02 3.48142785e-05 1.50495534e-05\n",
      "  2.09650034e-06]\n",
      " [9.96672630e-01 2.28793811e-04 8.84164037e-05 1.40496595e-05\n",
      "  2.99609173e-03]]\n",
      "final predictions: tf.Tensor([4 0 0 0 1], shape=(5,), dtype=int64) [4 0 0 4 1]\n",
      "accuracy on dev set: 0.9221919467151075\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model_2l.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# nonlinear_model_2l.fit(X_train, Y_train, batch_size=3000, epochs=80) # acc=0.8913\n",
    "# nonlinear_model_2l.fit(X_train, Y_train, batch_size=2000, epochs=80) # acc=0.8840\n",
    "nonlinear_model_2l.fit(X_train, Y_train, batch_size=1000, epochs=80) # acc=0.8565\n",
    "\n",
    "print('final loss:', nonlinear_model_2l.evaluate(X_dev, Y_dev))\n",
    "print('final responsibilities:', nonlinear_model_2l.predict(X_dev[:5]), sep='\\n')\n",
    "inds = np.random.choice(np.arange(len(Y_dev)), 5)\n",
    "print('final predictions:', tf.argmax(nonlinear_model_2l.predict(X_dev[inds]), axis=1), Y_dev[inds])\n",
    "print('accuracy on dev set:', (np.array(tf.argmax(nonlinear_model_2l.predict(X_dev), axis=1))==Y_dev).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13209 samples\n",
      "Epoch 1/200\n",
      "13209/13209 [==============================] - 0s 6us/sample - loss: 1.5740\n",
      "Epoch 2/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.4887\n",
      "Epoch 3/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.4187\n",
      "Epoch 4/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.3622\n",
      "Epoch 5/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.3159\n",
      "Epoch 6/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 1.2781\n",
      "Epoch 7/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.2463\n",
      "Epoch 8/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.2185\n",
      "Epoch 9/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.1940\n",
      "Epoch 10/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.1719\n",
      "Epoch 11/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.1516\n",
      "Epoch 12/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 1.1326\n",
      "Epoch 13/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.1147\n",
      "Epoch 14/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.0975\n",
      "Epoch 15/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.0809\n",
      "Epoch 16/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.0650\n",
      "Epoch 17/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 1.0496\n",
      "Epoch 18/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 1.0349\n",
      "Epoch 19/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 1.0205\n",
      "Epoch 20/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 1.0065\n",
      "Epoch 21/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.9932\n",
      "Epoch 22/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.9803\n",
      "Epoch 23/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.9679\n",
      "Epoch 24/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.9560\n",
      "Epoch 25/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.9446\n",
      "Epoch 26/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.9337\n",
      "Epoch 27/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.9232\n",
      "Epoch 28/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.9130\n",
      "Epoch 29/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.9033\n",
      "Epoch 30/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8939\n",
      "Epoch 31/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8849\n",
      "Epoch 32/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8762\n",
      "Epoch 33/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8680\n",
      "Epoch 34/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8600\n",
      "Epoch 35/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8523\n",
      "Epoch 36/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8448\n",
      "Epoch 37/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8377\n",
      "Epoch 38/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8307\n",
      "Epoch 39/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8239\n",
      "Epoch 40/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8173\n",
      "Epoch 41/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.8107\n",
      "Epoch 42/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.8044\n",
      "Epoch 43/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7983\n",
      "Epoch 44/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7923\n",
      "Epoch 45/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7865\n",
      "Epoch 46/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7808\n",
      "Epoch 47/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7753\n",
      "Epoch 48/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7700\n",
      "Epoch 49/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7649\n",
      "Epoch 50/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7598\n",
      "Epoch 51/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7550\n",
      "Epoch 52/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7502\n",
      "Epoch 53/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7457\n",
      "Epoch 54/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7413\n",
      "Epoch 55/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7370\n",
      "Epoch 56/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7327\n",
      "Epoch 57/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7285\n",
      "Epoch 58/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7245\n",
      "Epoch 59/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7206\n",
      "Epoch 60/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7168\n",
      "Epoch 61/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7131\n",
      "Epoch 62/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7095\n",
      "Epoch 63/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.7060\n",
      "Epoch 64/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.7026\n",
      "Epoch 65/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6992\n",
      "Epoch 66/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6959\n",
      "Epoch 67/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6928\n",
      "Epoch 68/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6898\n",
      "Epoch 69/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6868\n",
      "Epoch 70/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6838\n",
      "Epoch 71/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6810\n",
      "Epoch 72/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6783\n",
      "Epoch 73/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6756\n",
      "Epoch 74/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6730\n",
      "Epoch 75/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6704\n",
      "Epoch 76/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6679\n",
      "Epoch 77/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6655\n",
      "Epoch 78/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6631\n",
      "Epoch 79/200\n",
      "13209/13209 [==============================] - ETA: 0s - loss: 0.671 - 0s 2us/sample - loss: 0.6607\n",
      "Epoch 80/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6584\n",
      "Epoch 81/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6562\n",
      "Epoch 82/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6540\n",
      "Epoch 83/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6518\n",
      "Epoch 84/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.6497\n",
      "Epoch 85/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6476\n",
      "Epoch 86/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6456\n",
      "Epoch 87/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6435\n",
      "Epoch 88/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6416\n",
      "Epoch 89/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6396\n",
      "Epoch 90/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6377\n",
      "Epoch 91/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6358\n",
      "Epoch 92/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6339\n",
      "Epoch 93/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6321\n",
      "Epoch 94/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6302\n",
      "Epoch 95/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6284\n",
      "Epoch 96/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6265\n",
      "Epoch 97/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6248\n",
      "Epoch 98/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6230\n",
      "Epoch 99/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6213\n",
      "Epoch 100/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6196\n",
      "Epoch 101/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6178\n",
      "Epoch 102/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6162\n",
      "Epoch 103/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6145\n",
      "Epoch 104/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6129\n",
      "Epoch 105/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6112\n",
      "Epoch 106/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6096\n",
      "Epoch 107/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6080\n",
      "Epoch 108/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6064\n",
      "Epoch 109/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6048\n",
      "Epoch 110/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6032\n",
      "Epoch 111/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6017\n",
      "Epoch 112/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.6002\n",
      "Epoch 113/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5987\n",
      "Epoch 114/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5972\n",
      "Epoch 115/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5958\n",
      "Epoch 116/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5943\n",
      "Epoch 117/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5928\n",
      "Epoch 118/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5913\n",
      "Epoch 119/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5900\n",
      "Epoch 120/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5885\n",
      "Epoch 121/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5872\n",
      "Epoch 122/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5858\n",
      "Epoch 123/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5845\n",
      "Epoch 124/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5831\n",
      "Epoch 125/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5818\n",
      "Epoch 126/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5804\n",
      "Epoch 127/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5791\n",
      "Epoch 128/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5778\n",
      "Epoch 129/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5764\n",
      "Epoch 130/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5751\n",
      "Epoch 131/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5738\n",
      "Epoch 132/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5725\n",
      "Epoch 133/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5712\n",
      "Epoch 134/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5699\n",
      "Epoch 135/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5687\n",
      "Epoch 136/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5673\n",
      "Epoch 137/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5661\n",
      "Epoch 138/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5648\n",
      "Epoch 139/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5636\n",
      "Epoch 140/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5624\n",
      "Epoch 141/200\n",
      "13209/13209 [==============================] - 0s 5us/sample - loss: 0.5611\n",
      "Epoch 142/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5600\n",
      "Epoch 143/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5587\n",
      "Epoch 144/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5575\n",
      "Epoch 145/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5563\n",
      "Epoch 146/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5551\n",
      "Epoch 147/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5540\n",
      "Epoch 148/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5529\n",
      "Epoch 149/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5518\n",
      "Epoch 150/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5506\n",
      "Epoch 151/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5495\n",
      "Epoch 152/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5484\n",
      "Epoch 153/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5473\n",
      "Epoch 154/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5462\n",
      "Epoch 155/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5451\n",
      "Epoch 156/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5441\n",
      "Epoch 157/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5429\n",
      "Epoch 158/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5419\n",
      "Epoch 159/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5407\n",
      "Epoch 160/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5397\n",
      "Epoch 161/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5386\n",
      "Epoch 162/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5376\n",
      "Epoch 163/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5365\n",
      "Epoch 164/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5354\n",
      "Epoch 165/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5344\n",
      "Epoch 166/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5334\n",
      "Epoch 167/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5323\n",
      "Epoch 168/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5313\n",
      "Epoch 169/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5303\n",
      "Epoch 170/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5292\n",
      "Epoch 171/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5283\n",
      "Epoch 172/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5273\n",
      "Epoch 173/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5262\n",
      "Epoch 174/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5252\n",
      "Epoch 175/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5242\n",
      "Epoch 176/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5233\n",
      "Epoch 177/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5223\n",
      "Epoch 178/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5213\n",
      "Epoch 179/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5204\n",
      "Epoch 180/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5193\n",
      "Epoch 181/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5184\n",
      "Epoch 182/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5175\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5165\n",
      "Epoch 184/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5155\n",
      "Epoch 185/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5147\n",
      "Epoch 186/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5138\n",
      "Epoch 187/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5129\n",
      "Epoch 188/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5120\n",
      "Epoch 189/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5111\n",
      "Epoch 190/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5104\n",
      "Epoch 191/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5094\n",
      "Epoch 192/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5085\n",
      "Epoch 193/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5077\n",
      "Epoch 194/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5069\n",
      "Epoch 195/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5060\n",
      "Epoch 196/200\n",
      "13209/13209 [==============================] - 0s 2us/sample - loss: 0.5051\n",
      "Epoch 197/200\n",
      "13209/13209 [==============================] - 0s 4us/sample - loss: 0.5043\n",
      "Epoch 198/200\n",
      "13209/13209 [==============================] - 0s 5us/sample - loss: 0.5034\n",
      "Epoch 199/200\n",
      "13209/13209 [==============================] - 0s 5us/sample - loss: 0.5026\n",
      "Epoch 200/200\n",
      "13209/13209 [==============================] - 0s 3us/sample - loss: 0.5017\n",
      "3303/3303 [==============================] - 0s 27us/sample - loss: 0.4866\n",
      "final loss: 0.4865850584275283\n",
      "final responsibilities:\n",
      "[[1.58670992e-01 8.12314391e-01 1.40910575e-04 1.27879409e-02\n",
      "  1.60857458e-02]\n",
      " [8.07010293e-01 5.10007469e-03 1.41349446e-04 2.54613697e-04\n",
      "  1.87493607e-01]\n",
      " [6.04497211e-04 9.99382019e-01 2.65310092e-07 2.15647322e-08\n",
      "  1.32583782e-05]\n",
      " [7.67030835e-01 1.74440891e-01 9.17746002e-05 1.55404032e-05\n",
      "  5.84209189e-02]\n",
      " [8.23620498e-01 4.85591888e-02 1.15206196e-04 3.06747097e-05\n",
      "  1.27674520e-01]]\n",
      "final predictions: tf.Tensor([1 0 1 0 0], shape=(5,), dtype=int64) [1 0 1 0 0]\n",
      "accuracy on dev set: 0.8104753254617015\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model_2l_10 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(9,), activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model_2l_10.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "# nonlinear_model_2l.fit(X_train, Y_train, batch_size=3000, epochs=40) # acc=0.7914\n",
    "# nonlinear_model_2l.fit(X_train, Y_train, batch_size=2000, epochs=40) # acc=0.8062\n",
    "nonlinear_model_2l_10.fit(X_train, Y_train, batch_size=1000, epochs=200) # acc=0.8392\n",
    "\n",
    "print('final loss:', nonlinear_model_2l_10.evaluate(X_dev, Y_dev))\n",
    "print('final responsibilities:', nonlinear_model_2l_10.predict(X_dev[:5]), sep='\\n')\n",
    "print('final predictions:', tf.argmax(nonlinear_model_2l_10.predict(X_dev[:5]), axis=1), Y_dev[:5])\n",
    "print('accuracy on dev set:', (np.array(tf.argmax(nonlinear_model_2l_10.predict(X_dev), axis=1))==Y_dev).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3303/3303 [==============================] - 0s 26us/sample - loss: 0.1525\n",
      "final loss: 0.1524726086553573\n",
      "final responsibilities:\n",
      "[[3.9775503e-05 9.9996006e-01 2.0025535e-09 9.1685941e-08 6.1773964e-10]\n",
      " [9.9034274e-01 3.1162926e-05 2.1506421e-04 1.9875783e-07 9.4107315e-03]\n",
      " [1.3791361e-06 9.9999857e-01 3.0815483e-10 1.5014988e-08 2.3488120e-10]\n",
      " [9.9972218e-01 1.7338974e-04 1.3641670e-05 3.5361246e-08 9.0792513e-05]\n",
      " [9.8702186e-01 5.8072623e-05 1.9058556e-04 1.1102935e-07 1.2729295e-02]]\n",
      "final predictions: tf.Tensor([1 0 1 0 0], shape=(5,), dtype=int64) [1 0 1 0 0]\n",
      "accuracy on train set: 0.9330759330759331\n",
      "accuracy on dev set: 0.9361186799878898\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model_3l = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(9,), activation='tanh'),\n",
    "    tf.keras.layers.Dense(20, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model_3l.compile(optimizer=tf.keras.optimizers.Ftrl(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "nonlinear_model_3l.fit(X_train, Y_train, batch_size=3000, epochs=1000, verbose=0) # acc=0.7914\n",
    "# nonlinear_model_3l.fit(X_train, Y_train, batch_size=2000, epochs=40) # acc=0.8062\n",
    "# nonlinear_model_3l.fit(X_train, Y_train, batch_size=1000, epochs=80) # acc=0.8392\n",
    "\n",
    "print('final loss:', nonlinear_model_3l.evaluate(X_dev, Y_dev))\n",
    "print('final responsibilities:', nonlinear_model_3l.predict(X_dev[:5]), sep='\\n')\n",
    "print('final predictions:', tf.argmax(nonlinear_model_3l.predict(X_dev[:5]), axis=1), Y_dev[:5])\n",
    "print('accuracy on train set:', (np.array(tf.argmax(nonlinear_model_3l.predict(X_train), axis=1))==Y_train).mean())\n",
    "print('accuracy on dev set:', (np.array(tf.argmax(nonlinear_model_3l.predict(X_dev), axis=1))==Y_dev).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3303/3303 [==============================] - 0s 39us/sample - loss: 0.3663\n",
      "final loss: 0.3662685560451072\n",
      "final responsibilities:\n",
      "[[6.5618856e-03 9.9065650e-01 2.2857690e-04 2.1495228e-03 4.0347278e-04]\n",
      " [9.7588718e-01 2.8703150e-03 4.3732007e-06 3.2193452e-04 2.0916145e-02]\n",
      " [5.8905976e-03 9.9140888e-01 2.2711218e-04 2.0906748e-03 3.8264907e-04]\n",
      " [9.3498063e-01 5.8943801e-02 2.7389069e-05 7.4532663e-04 5.3028637e-03]\n",
      " [9.7716522e-01 1.5107216e-02 1.1321932e-05 4.4966702e-04 7.2665447e-03]]\n",
      "final predictions: tf.Tensor([1 0 1 0 0], shape=(5,), dtype=int64) [1 0 1 0 0]\n",
      "accuracy on train set: 0.8567643273525627\n",
      "accuracy on dev set: 0.8546775658492279\n"
     ]
    }
   ],
   "source": [
    "nonlinear_model_3lL = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(18, input_shape=(9,), activation='tanh'),\n",
    "    tf.keras.layers.Dense(25, activation='softplus'),\n",
    "    tf.keras.layers.Dense(20, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "nonlinear_model_3lL.compile(optimizer=tf.keras.optimizers.Ftrl(learning_rate=1),\n",
    "                        loss='sparse_categorical_crossentropy')\n",
    "nonlinear_model_3lL.fit(X_train, Y_train, batch_size=3000, epochs=100, verbose=0) # acc=0.7914\n",
    "# nonlinear_model_3lL.fit(X_train, Y_train, batch_size=2000, epochs=40) # acc=0.8062\n",
    "# nonlinear_model_3lL.fit(X_train, Y_train, batch_size=1000, epochs=80) # acc=0.8392\n",
    "\n",
    "print('final loss:', nonlinear_model_3lL.evaluate(X_dev, Y_dev))\n",
    "print('final responsibilities:', nonlinear_model_3lL.predict(X_dev[:5]), sep='\\n')\n",
    "print('final predictions:', tf.argmax(nonlinear_model_3lL.predict(X_dev[:5]), axis=1), Y_dev[:5])\n",
    "print('accuracy on train set:', (np.array(tf.argmax(nonlinear_model_3lL.predict(X_train), axis=1))==Y_train).mean())\n",
    "print('accuracy on dev set:', (np.array(tf.argmax(nonlinear_model_3lL.predict(X_dev), axis=1))==Y_dev).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
